"use strict";(self.webpackChunkmaritaca=self.webpackChunkmaritaca||[]).push([[414],{3672:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var a=t(4848),i=t(8453);const r={id:"Sabia-3+RAG",title:"Sabia-3+RAG"},s="RAG with the Sabi\xe1-3",o={id:"en/Sabia-3+RAG",title:"Sabia-3+RAG",description:"RAG (Retrieval-Augmented Generation) is an approach that combines information retrieval from a database with the text generation capabilities of a language model (LLM). Below is a detailed step-by-step guide to creating a RAG system using the Sabia-3 model. In this example, we will use:",source:"@site/docs/en/Sabia-3+RAG.md",sourceDirName:"en",slug:"/en/Sabia-3+RAG",permalink:"/en/en/Sabia-3+RAG",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"Sabia-3+RAG",title:"Sabia-3+RAG"},sidebar:"sidebarEn",previous:{title:"Function Calls",permalink:"/en/en/function-call"},next:{title:"Embeddings",permalink:"/en/en/embeddings"}},l={},c=[{value:"Step 1: Install Dependencies",id:"step-1-install-dependencies",level:2},{value:"Step 2: Download the PDF",id:"step-2-download-the-pdf",level:2},{value:"Step 3: Load and Process the Document",id:"step-3-load-and-process-the-document",level:2},{value:"Step 4: Configure Sabia-3",id:"step-4-configure-sabia-3",level:2},{value:"Step 5: Define the Prompt",id:"step-5-define-the-prompt",level:2},{value:"Step 6: Create the Question-Answer Chain",id:"step-6-create-the-question-answer-chain",level:2},{value:"Step 7: Execute the Query",id:"step-7-execute-the-query",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"rag-with-the-sabi\xe1-3",children:"RAG with the Sabi\xe1-3"})}),"\n",(0,a.jsx)(n.p,{children:"RAG (Retrieval-Augmented Generation) is an approach that combines information retrieval from a database with the text generation capabilities of a language model (LLM). Below is a detailed step-by-step guide to creating a RAG system using the Sabia-3 model. In this example, we will use:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"A data source (a PDF of a Unicamp public notice)."}),"\n",(0,a.jsx)(n.li,{children:"A tool to extract and split the text from the PDF into chunks."}),"\n",(0,a.jsx)(n.li,{children:"A retrieval system (retriever) based on BM25, which searches for the most relevant excerpts for a given question."}),"\n",(0,a.jsx)(n.li,{children:"The Sabia-3 model to generate the final answer."}),"\n",(0,a.jsx)(n.li,{children:"A prompt in a conversational format, providing context and the user's question."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"step-1-install-dependencies",children:"Step 1: Install Dependencies"}),"\n",(0,a.jsx)(n.p,{children:"First, install the libraries needed for the project."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'!pip install unstructured rank_bm25 pdf2image pdfminer-six pikepdf pypdf unstructured_inference "pillow<10.1.0" pillow_heif -q\n!pip install langchain langchain-community langchain_openai -q\n'})}),"\n",(0,a.jsx)(n.h2,{id:"step-2-download-the-pdf",children:"Step 2: Download the PDF"}),"\n",(0,a.jsx)(n.p,{children:"In this example, we use a public notice from Unicamp. To download it locally:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"!wget https://www.comvest.unicamp.br/wp-content/uploads/2023/10/31-2023-Dispoe-sobre-o-Vestibular-Unicamp-2024_com-retificacao.pdf -O edital_unicamp_2024.pdf\n"})}),"\n",(0,a.jsx)(n.h2,{id:"step-3-load-and-process-the-document",children:"Step 3: Load and Process the Document"}),"\n",(0,a.jsx)(n.p,{children:"We will use LangChain's PyPDFLoader to extract the text and then split the text into smaller chunks using RecursiveCharacterTextSplitter."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Carregar o PDF\nloader = PyPDFLoader("edital_unicamp_2024.pdf")\ndata = loader.load()\n\n# Dividir o texto em chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=100,\n    separators=["\\n", " ", ""]\n)\ntexts = text_splitter.split_documents(data)\n\n'})}),"\n",(0,a.jsx)(n.p,{children:"With the text in chunks, we create a BM25 retriever to find the most relevant excerpts:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from langchain_community.retrievers import BM25Retriever\n\nretriever = BM25Retriever.from_documents(texts)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"step-4-configure-sabia-3",children:"Step 4: Configure Sabia-3"}),"\n",(0,a.jsx)(n.p,{children:"Here, we use compatibility with OpenAI through ChatOpenAI to access the Sabi\xe1-3 model. It is necessary to provide the base_url and the api_key."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(\n    model="sabia-3",\n    temperature=0,\n    api_key=api_key, # Enter your key here\n    base_url="https://chat.maritaca.ai/api",\n)\n\n'})}),"\n",(0,a.jsx)(n.h2,{id:"step-5-define-the-prompt",children:"Step 5: Define the Prompt"}),"\n",(0,a.jsx)(n.p,{children:"We will create a simple prompt that provides the context (retrieved documents) and the user's question."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate(\n    input_variables=["context", "query"],\n    template="Baseado nos seguintes documentos, responda a pergunta abaixo.\\n\\n{context}\\n\\nPergunta: {query}"\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"step-6-create-the-question-answer-chain",children:"Step 6: Create the Question-Answer Chain"}),"\n",(0,a.jsx)(n.p,{children:"Now we create the QA chain using LangChain's load_qa_chain."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chains.question_answering import load_qa_chain\n\nchain = load_qa_chain(llm, chain_type="stuff", prompt=prompt, verbose=True)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"step-7-execute-the-query",children:"Step 7: Execute the Query"}),"\n",(0,a.jsx)(n.p,{children:"Finally, we will ask the system a question. First, we retrieve the relevant documents with the retriever, then pass everything to the chain."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'query = "Qual o tempo m\xe1ximo para realiza\xe7\xe3o da prova?"\ndocs = retriever.invoke(query)\n\ntry:\n    answer = chain({"input_documents": docs, "query": query})\n    print(answer)\nexcept Exception as e:\n    print(f"Erro durante execu\xe7\xe3o: {e}")\n\n'})}),"\n",(0,a.jsx)(n.p,{children:"The query sent to the system will return a result in JSON format, structured as follows:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "input_documents": [\n    {\n      "metadata": {\n        "source": "edital_unicamp_2024.pdf",\n        "page": 8\n      },\n      "page_content": "M\xe9dio, incluindo quest\xf5es interdisciplinares. \\n\\n\xa71\xba O(a) candidato(a) ter\xe1 no m\xe1ximo 5 (cinco) horas e no m\xednimo 2 duas horas para a \\nrealiza\xe7\xe3o da prova da 1\xaa fase. Poder\xe1 ser concedido tempo adicional aos(\xe0s) candidatos(as) nos \\ncasos previstos no art. 14."\n    },\n    {\n      "metadata": {\n        "source": "edital_unicamp_2024.pdf",\n        "page": 78\n      },\n      "page_content": "c. Com maior tempo para a realiza\xe7\xe3o da prova, tempo este estabelecido de acordo com \\ncrit\xe9rios neuropsicol\xf3gicos, at\xe9 o limite de 20% do tempo regular."\n    }\n    // ... outros documentos\n  ],\n  "query": "Qual o tempo m\xe1ximo para realiza\xe7\xe3o da prova?",\n  "output_text": "O tempo m\xe1ximo para a realiza\xe7\xe3o da prova \xe9 de 5 (cinco) horas."\n}\n\n'})})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var a=t(6540);const i={},r=a.createContext(i);function s(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);