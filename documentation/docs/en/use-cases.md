---
id: use-cases
title: Use Cases
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Use Cases
### Streaming
For long text generation tasks, such as creating an extensive article or translating a large document, it can be advantageous to receive the response in parts as the text is generated, rather than waiting for the full text. This makes the application more responsive and efficient, especially when the generated text is lengthy. We offer two approaches to meet this need: using a generator and an async_generator.

#### Generator
- When using `stream=True`, the code will return a `generator`. This `generator` will provide parts of the response as they are generated by the model, allowing you to print or process the tokens as they are produced.

```python
for response in model.generate(
    messages,
    do_sample=True,
    max_tokens=200,
    temperature=0.7,
    top_p=0.95,
    stream=True,
    num_tokens_per_message=4
):
    print(response)
```

#### AsyncGenerator
When using `stream=True` together with `return_async_generator=True`, the code will return an `AsyncGenerator`. This type of generator is designed to be consumed asynchronously, which means you can run the code that consumes the `AsyncGenerator` concurrently with other tasks, improving your processing efficiency.

```python
import asyncio

async_generator = model.generate(
    messages,
    do_sample=True,
    max_tokens=200,
    temperature=0.7,
    top_p=0.95,
    stream=True,
    return_async_generator=True,
    num_tokens_per_message=4
)

async def consume_generator():
    async for response in async_generator:
        print(response)
        # Your code here...

asyncio.run(consume_generator)
```

## Chat Mode

You can define a conversation by specifying a list of dictionaries, where each dictionary must have two keys: `content` and `role`.

Currently, the Maritaca API supports three values for `role`: "system" for chatbot instruction messages, "user" for user messages, and "assistant" for assistant messages.

Below is an example of a conversation:
```bash
messages = [
    {"role": "user", "content": "sugira três nomes para a minha cachorra"},
    {"role": "assistant", "content": "nina, bela e luna."},
    {"role": "user", "content": "e para o meu peixe?"},
]

answer = model.generate(
    messages,
    do_sample=True,
    max_tokens=200,
    temperature=0.7,
    top_p=0.95)["answer"]

print(f"Resposta: {answer}")   # Should print something like "nemo, dory e neptuno."
```

## Few-shot Examples

Although Sabiá is capable of responding to instructions without any demonstration examples, providing a few examples of the task can significantly improve the quality of its responses.

Below we show how this is done for a simple sentiment analysis task, i.e., classifying whether a movie review is positive or negative.
In this case, we will pass two few-shot examples, one positive and one negative, and a third example, for which Sabiá will effectively make the prediction.
```python
prompt = """Classifique a resenha de filme como "positiva" ou "negativa".

Resenha: Gostei muito do filme, é o melhor do ano!
Classe: positiva

Resenha: O filme deixa muito a desejar.
Classe: negativa

Resenha: Apesar de longo, valeu o ingresso..
Classe:"""

answer = model.generate(
    prompt,
    chat_mode=False,
    do_sample=False,
    max_tokens=20,
    stopping_tokens=["\n"]
)["answer"]

print(f"Resposta: {answer.strip()}")  # Should print "positiva"
```

Note that we used `chat_mode=False`, as it improves the quality of responses when using few-shot examples.

The argument `stopping_tokens=["\n"]` is used to stop the generation when the token "\n" is generated. This is necessary because, when not in chat mode, the model may not know when to stop generating.

For tasks with only one correct answer, such as in the example above, it is recommended to use `do_sample=False`. This ensures that the same response is generated given a specific prompt.

For tasks involving the generation of diverse or long texts, it is recommended to use `do_sample=True` and `temperature=0.7`. The higher the temperature, the more diverse the generated texts will be, but there is a higher chance of the model "hallucinating" and generating nonsensical texts. The lower the temperature, the more conservative the response, but there is a risk of generating repeated texts.

## Usage Examples in Other Languages
Here are examples of how you can integrate Maritaca's API in other languages:
<Tabs>
<TabItem value="JavaScript" label="JavaScript" default>
```javascript
const process = require('node:process');

const CHAT_API_URL = "https://chat.maritaca.ai/api/chat/inference";

if (!process.env.MARITALK_API_KEY) {
    console.error("Environment variable MARITALT_API_KEY not found!");
    process.exit(1);
}

async function sendChatRequest(message) {
    try {
        const params = {
            messages: [{ "role": "user", "content": message }],
            do_sample: true,
            max_tokens: 50,
            temperature: 0.4,
            top_p: 0.95,
            model: "sabia-3",
        };

        const response = await fetch(CHAT_API_URL, {
            headers: {
                "Authorization": `Key ${process.env.MARITALK_API_KEY}`,
                "Content-Type": "application/json",
            },
            method: "POST",
            body: JSON.stringify(params),
        });

        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }

        return await response.json();
    } catch (error) {
        console.error("Error sending chat request:", error);
        throw error;
    }
}

async function main() {
    try {
        const result = await sendChatRequest('Olá, qual é seu nome?');
        console.log("Response:", result);
    } catch (error) {
        console.error("Error in main function:", error);
    }
}

main();
```
</TabItem>
<TabItem value="C#" label="C#">
```csharp
using OpenAI;
using OpenAI.Chat;
using System.ClientModel;

namespace ChatMariTalk
{
    class Program
    {
        static async Task Main(string[] args)
        {
            //variables
            string key = "";
            string model = "sabia-3";
            string url = "https://chat.maritaca.ai/api";
            string nameProject = "ExemploUsandoMaritaca";

            //Create the credential using the API access key
            ApiKeyCredential apiKeyCredential = new ApiKeyCredential(key);

            //Configure the Client for the Maritaca endpoint
            OpenAIClientOptions openAIClientOptions = new OpenAIClientOptions
            {
                Endpoint = new Uri(url),
                OrganizationId = nameProject,
                ApplicationId = nameProject,
                ProjectId = nameProject
            };

            //Create the ChatClient
            ChatClient chatClient = new ChatClient(model, apiKeyCredential, openAIClientOptions);

            //Create the chat options
            ChatCompletionOptions chatOptions = new ChatCompletionOptions
            {
                MaxTokens = 255,
                Temperature = 0.7f,
            };

            //Create a list to store the chat messages
            List<ChatMessage> chatMessages = new List<ChatMessage>();

            //Show the menu
            ShowMenu();
            do
            {
                try
                {
                    //User question
                    string prompt = ShowPrompt();

                    //If the user types 'sair', the chat ends
                    if (prompt.ToLower() == "sair")
                        break;

                    //Create the user message
                    UserChatMessage userChat = ChatMessage.CreateUserMessage(prompt);
                    chatMessages.Add(userChat);

                    //Send the question to the API
                    ChatCompletion chatCompletion = await chatClient.CompleteChatAsync(chatMessages, chatOptions);

                    //Capture the response from the model
                    AssistantChatMessage assistant = ChatMessage.CreateAssistantMessage(chatCompletion.Content[0].Text);
                    chatMessages.Add(assistant);

                    //Display the model's response
                    ShowAssistant(assistant);
                }
                catch (Exception e)
                {
                    ShowError(e);
                }
            } while (true);

            ShowSair();
        }

        static void ShowMenu()
        {
            Console.WriteLine(new string('-', 100));
            Console.WriteLine();
            Console.WriteLine("Bem-vindo ao Maritaca Chat!");
            Console.WriteLine();
            Console.WriteLine(new string('-', 100));
            Console.WriteLine("Digite 'sair' para encerrar o chat.");
            Console.WriteLine();
            Console.WriteLine();
            Console.WriteLine();
        }

        static string ShowPrompt()
        {
            Console.ForegroundColor = ConsoleColor.Green;
            Console.Write($"[{DateTime.Now}] Envie uma mensagem: ");
            Console.ResetColor();
            string prompt = Console.ReadLine();
            if (string.IsNullOrEmpty(prompt))
            {
                throw new Exception("Por favor, insira uma mensagem.");
            }
            return prompt;
        }

        static void ShowAssistant(AssistantChatMessage assistant)
        {
            Console.ForegroundColor = ConsoleColor.Yellow;
            Console.WriteLine();
            Console.Write($"[{DateTime.Now}] Assistente: ");
            Console.ResetColor();
            Console.WriteLine(assistant.Content[0].Text);
            Console.WriteLine();
        }

        static void ShowError(Exception e)
        {
            Console.WriteLine();
            Console.ForegroundColor = ConsoleColor.Red;
            Console.Write($"[{DateTime.Now}] Erro: ");
            Console.ResetColor();
            Console.WriteLine(e.Message);
            Console.WriteLine();
        }

        static void ShowSair()
        {
            Console.WriteLine(new string('-', 100));
            Console.WriteLine();
            Console.WriteLine("Obrigado por usar o Maritaca Chat!");
            Console.WriteLine();
            Console.WriteLine();
        }
    }
}
```
</TabItem>
</Tabs>

## Integrations
<div style={{ display: 'flex', justifyContent: 'space-around', margin: '20px 0', flexWrap: 'wrap' }}>
  <a href="https://docs.llamaindex.ai/en/latest/examples/llm/maritalk/" className="icon-box" style={{ flex: '1 1 200px', margin: '10px', textAlign: 'center' }}>
    <i className="fas fa-file-alt" style={{ fontSize: '2em', marginBottom: '10px' }}></i> 
    <h3>LlamaIndex</h3>
    <p>Maritalk in LlamaIndex.</p>
  </a>
  <a href="https://python.langchain.com/v0.2/docs/integrations/chat/maritalk/" className="icon-box" style={{ flex: '1 1 200px', margin: '10px', textAlign: 'center' }}>
    <i className="fas fa-link" style={{ fontSize: '2em', marginBottom: '10px' }}></i> 
    <h3>LangChain</h3>
    <p>MariTalk + RAG with LangChain.</p>
  </a>
</div>
