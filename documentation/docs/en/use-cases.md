---
id: use-cases
title: Use Cases
---
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';


# Use Cases
### Streaming
For tasks involving the generation of long text, such as writing an extensive article or translating a large document, it can be beneficial to receive the response in parts as the text is generated rather than waiting for the full text. This makes the application more responsive and efficient, especially when dealing with extensive generated text. We offer two approaches to meet this need: using a generator and an async_generator.

#### Generator
- When `stream=True` is used, the code will return a `generator`. This `generator` will provide parts of the response as they are generated by the model, allowing you to print or process the tokens as they are produced.

```python
for response in model.generate(
    messages,
    do_sample=True,
    max_tokens=200,
    temperature=0.7,
    top_p=0.95,
    stream=True,
    num_tokens_per_message=4
):
    print(response)
```

#### AsyncGenerator
Using `stream=True` together with `return_async_generator=True`, the code will return an `AsyncGenerator`. This type of generator is designed to be consumed asynchronously, meaning you can run the code that consumes the `AsyncGenerator` concurrently with other tasks, improving your processing efficiency.

```python
import asyncio

async_generator = model.generate(
    messages,
    do_sample=True,
    max_tokens=200,
    temperature=0.7,
    top_p=0.95,
    stream=True,
    return_async_generator=True,
    num_tokens_per_message=4
)

async def consume_generator():
    async for response in async_generator:
        print(response)
        # Your code here...

asyncio.run(consume_generator)
```

## Chat Mode

You can define a conversation by specifying a list of dictionaries, where each dictionary needs to have two keys: `content` and `role`.

Currently, Maritaca's API supports three values for `role`: "system" for chatbot instruction messages, "user" for user messages, and "assistant" for assistant messages.

Below is an example of a conversation:
```bash
messages = [
    {"role": "user", "content": "suggest three names for my dog"},
    {"role": "assistant", "content": "nina, bela and luna."},
    {"role": "user", "content": "and for my fish?"},
]

answer = model.generate(
    messages,
    do_sample=True,
    max_tokens=200,
    temperature=0.7,
    top_p=0.95)["answer"]

print(f"Answer: {answer}")   # Should print something like "nemo, dory and neptune."
```

## Few-shot Examples

Although Sabiá is capable of responding to instructions without any demonstration examples, providing a few examples of the task can significantly improve the quality of its responses.

Below we show how this is done for a simple sentiment analysis task, i.e., classifying whether a movie review is positive or negative.
In this case, we will pass two few-shot examples, one positive and one negative, and a third example for which Sabiá will actually make the prediction.
```python
prompt = """Classify the movie review as "positive" or "negative".

Review: I really liked the movie, it's the best of the year!
Class: positive

Review: The movie leaves much to be desired.
Class: negative

Review: Despite being long, it was worth the ticket..
Class:"""

answer = model.generate(
    prompt,
    chat_mode=False,
    do_sample=False,
    max_tokens=20,
    stopping_tokens=["\n"]
)["answer"]

print(f"Answer: {answer.strip()}")  # Should print "positive"
```

Note that we used `chat_mode=False`, as it improves the quality of the responses when using few-shot examples.

The argument `stopping_tokens=["\n"]` is used to stop the generation when the token "\n" is generated. This is necessary because, when not in chat mode, the model may not know when to stop generating.

For tasks with only one correct answer, like in the example above, it is recommended to use `do_sample=False`. This ensures that the same answer is generated given a specific prompt.

For tasks involving the generation of diverse or long texts, it is recommended to use `do_sample=True` and `temperature=0.7`. The higher the temperature, the more diverse the generated texts will be, but there is a higher chance of the model "hallucinating" and generating nonsensical texts. The lower the temperature, the more conservative the response, but there is a risk of generating repetitive texts.

## Examples of Usage in Other Languages
Here are examples of how you can integrate Maritaca's API into other languages:
<Tabs>
<TabItem value="JavaScript" label="JavaScript" default>
```javascript
const process = require('node:process');

const CHAT_API_URL = "https://chat.maritaca.ai/api/chat/inference";

if (!process.env.MARITALK_API_KEY) {
    console.error("Environment variable MARITALK_API_KEY not found!");
    process.exit(1);
}

async function sendChatRequest(message) {
    try {
        const params = {
            messages: [{ "role": "user", "content": message }],
            do_sample: true,
            max_tokens: 50,
            temperature: 0.4,
            top_p: 0.95,
            model: "sabia-2-medium",
        };

        const response = await fetch(CHAT_API_URL, {
            headers: {
                "Authorization": `Key ${process.env.MARITALK_API_KEY}`,
                "Content-Type": "application/json",
            },
            method: "POST",
            body: JSON.stringify(params),
        });

        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }

        return await response.json();
    } catch (error) {
        console.error("Error sending chat request:", error);
        throw error;
    }
}

async function main() {
    try {
        const result = await sendChatRequest('Hello, what is your name?');
        console.log("Response:", result);
    } catch (error) {
        console.error("Error in main function:", error);
    }
}

main();
```
</TabItem>
<TabItem value="C#" label="C#">
```csharp
using OpenAI;
using OpenAI.Chat;
using System.ClientModel;

namespace ChatMariTalk
{
    class Program
    {
        static async Task Main(string[] args)
        {
            //variables
            string key = "";
            string model = "sabia-3";
            string url = "https://chat.maritaca.ai/api";
            string nameProject = "ExampleUsingMaritaca";

            //Create the credential using the API access key
            ApiKeyCredential apiKeyCredential = new ApiKeyCredential(key);

            //Configure the Client for the Maritaca endpoint
            OpenAIClientOptions openAIClientOptions = new OpenAIClientOptions
            {
                Endpoint = new Uri(url),
                OrganizationId = nameProject,
                ApplicationId = nameProject,
                ProjectId = nameProject
            };

            //Create the ChatClient
            ChatClient chatClient = new ChatClient(model, apiKeyCredential, openAIClientOptions);

            //Create the chat options
            ChatCompletionOptions chatOptions = new ChatCompletionOptions
            {
                MaxTokens = 255,
                Temperature = 0.7f,
            };

            //Create a list to store the chat messages
            List<ChatMessage> chatMessages = new List<ChatMessage>();

            //Show the menu
            ShowMenu();
            do
            {
                try
                {
                    //User question
                    string prompt = ShowPrompt();

                    //If the user types 'exit', the chat ends
                    if (prompt.ToLower() == "exit")
                        break;

                    //Create the user message
                    UserChatMessage userChat = ChatMessage.CreateUserMessage(prompt);
                    chatMessages.Add(userChat);

                    //Send the question to the API
                    ChatCompletion chatCompletion = await chatClient.CompleteChatAsync(chatMessages, chatOptions);

                    //Capture the response from the model
                    AssistantChatMessage assistant = ChatMessage.CreateAssistantMessage(chatCompletion.Content[0].Text);
                    chatMessages.Add(assistant);

                    //Display the model's response
                    ShowAssistant(assistant);
                }
                catch (Exception e)
                {
                    ShowError(e);
                }
            } while (true);

            ShowExit();
        }

        // Methods for ShowMenu, ShowPrompt, ShowAssistant, ShowError, ShowExit would be translated similarly...
        // ...
    }
}
```
</TabItem>
</Tabs>

## Integrations
<div style={{ display: 'flex', justifyContent: 'space-around', margin: '20px 0', flexWrap: 'wrap' }}>
  <a href="https://docs.llamaindex.ai/en/latest/examples/llm/maritalk/" className="icon-box" style={{ flex: '1 1 200px', margin: '10px', textAlign: 'center' }}>
    <i className="fas fa-file-alt" style={{ fontSize: '2em', marginBottom: '10px' }}></i> 
    <h3>LlamaIndex</h3>
    <p>Maritalk in LlamaIndex.</p>
  </a>
  <a href="https://python.langchain.com/v0.2/docs/integrations/chat/maritalk/" className="icon-box" style={{ flex: '1 1 200px', margin: '10px', textAlign: 'center' }}>
    <i className="fas fa-link" style={{ fontSize: '2em', marginBottom: '10px' }}></i> 
    <h3>LangChain</h3>
    <p>MariTalk + RAG with LangChain.</p>
  </a>
</div>
