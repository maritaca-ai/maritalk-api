"use strict";(self.webpackChunkmaritaca=self.webpackChunkmaritaca||[]).push([[425],{2672:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>c});var r=n(4848),o=n(8453);const t={id:"oracle-cloud",title:"Oracle Cloud"},i="Executando MariTalk Local na Oracle Cloud Infrastructure (OCI)",s={id:"pt/maritalk-local/oracle-cloud",title:"Oracle Cloud",description:"Este tutorial mostra como executar a MariTalk Local na Oracle Cloud Infrastructure (OCI). Para isso, utilizaremos uma inst\xe2ncia com a GPU NVIDIA A10 24GB (VM.GPU.A10.1). At\xe9 o momento, o software tamb\xe9m foi testado nas GPUs NVIDIA L4 e A100, mas \xe9 esperado que funcione em outras GPUs com compute capability 8.0+ (nvidia-smi --query-gpu=compute_cap --format=csv).",source:"@site/docs/pt/maritalk-local/oracle-cloud.md",sourceDirName:"pt/maritalk-local",slug:"/pt/maritalk-local/oracle-cloud",permalink:"/pt/maritalk-local/oracle-cloud",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"oracle-cloud",title:"Oracle Cloud"},sidebar:"sidebarPt",previous:{title:"Google Cloud",permalink:"/pt/maritalk-local/google-cloud"},next:{title:"Docker",permalink:"/pt/maritalk-local/docker"}},l={},c=[];function d(e){const a={a:"a",code:"code",h1:"h1",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(a.header,{children:(0,r.jsx)(a.h1,{id:"executando-maritalk-local-na-oracle-cloud-infrastructure-oci",children:"Executando MariTalk Local na Oracle Cloud Infrastructure (OCI)"})}),"\n",(0,r.jsxs)(a.p,{children:["Este tutorial mostra como executar a MariTalk Local na Oracle Cloud Infrastructure (OCI). Para isso, utilizaremos uma inst\xe2ncia com a GPU NVIDIA A10 24GB (",(0,r.jsx)(a.code,{children:"VM.GPU.A10.1"}),"). At\xe9 o momento, o software tamb\xe9m foi testado nas GPUs NVIDIA L4 e A100, mas \xe9 esperado que funcione em outras GPUs com ",(0,r.jsx)(a.a,{href:"https://developer.nvidia.com/cuda-gpus",children:"compute capability"})," 8.0+ (",(0,r.jsx)(a.code,{children:"nvidia-smi --query-gpu=compute_cap --format=csv"}),")."]}),"\n",(0,r.jsx)(a.p,{children:"Para executar a vers\xe3o Small, as m\xe1quinas precisam de pelo menos 32 GB de mem\xf3ria de CPU e uma GPU com 24 GB de mem\xf3ria. Para a vers\xe3o Medium, \xe9 necess\xe1rio um m\xednimo de 130 GB de mem\xf3ria de CPU e pelo menos 70 GB de mem\xf3ria de GPU, que podem ser distribu\xeddos em mais de um dispositivo. Por exemplo, pode-se usar 2 GPUs A100 de 40 GB, 1 GPU A100 de 80 GB ou 4 GPUs A10 de 24 GB."}),"\n",(0,r.jsxs)(a.p,{children:["Voc\xea pode adquirir uma licen\xe7a da MariTalk Local ",(0,r.jsx)(a.a,{href:"https://maritaca.ai/#maritalk-local",children:"neste link"}),"."]}),"\n",(0,r.jsxs)(a.ol,{children:["\n",(0,r.jsxs)(a.li,{children:["Crie uma inst\xe2ncia na OCI selecionando ",(0,r.jsx)(a.code,{children:"NVIDIA GPU Cloud Machine Image"})," como imagem e ",(0,r.jsx)(a.code,{children:"VM.GPU.A10.1"})," como shape."]}),"\n"]}),"\n",(0,r.jsx)("img",{src:"/img/oracle-screenshot.png",alt:"oracle"}),"\n",(0,r.jsxs)(a.ol,{start:"2",children:["\n",(0,r.jsx)(a.li,{children:"Verifique se a GPU foi detectada com sucesso."}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{children:"$ nvidia-smi\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA A10                      On | 00000000:00:04.0 Off |                    0 |\n|  0%   38C    P8               21W / 150W|      0MiB / 23028MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n"})}),"\n",(0,r.jsxs)(a.p,{children:["Alternativamente, voc\xea pode usar um ",(0,r.jsx)(a.a,{href:"/pt/maritalk-local/docker",children:"container Docker"})," para configurar a aplica\xe7\xe3o."]}),"\n",(0,r.jsxs)(a.ol,{start:"3",children:["\n",(0,r.jsxs)(a.li,{children:["Instale as depend\xeancias necess\xe1rias utilizando o gerenciador de pacotes ",(0,r.jsx)(a.code,{children:"apt"}),". S\xe3o necess\xe1rios os pacotes ",(0,r.jsx)(a.code,{children:"cuda-toolkit-12"})," e ",(0,r.jsx)(a.code,{children:"libnccl2"}),"."]}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"sudo apt update\nsudo apt-get install -y libnccl2 cuda-toolkit-12\n"})}),"\n",(0,r.jsxs)(a.ol,{start:"4",children:["\n",(0,r.jsx)(a.li,{children:"Instale a biblioteca Python para interagir com o servidor da MariTalk Local."}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-bash",children:"python3 -m pip install maritalk\n"})}),"\n",(0,r.jsxs)(a.ol,{start:"5",children:["\n",(0,r.jsxs)(a.li,{children:["Voc\xea pode iniciar o servidor da MariTalk Local manualmente ou utilizar o m\xe9todo ",(0,r.jsx)(a.code,{children:"start_server"})," para fazer o download e iniciar o servidor automaticamente. Abra um console Python (",(0,r.jsx)(a.code,{children:"$ python3"}),") para iniciar o servidor e comece a testar!"]}),"\n"]}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-python",children:'>>> import maritalk\n>>> client = maritalk.MariTalkLocal()\n>>> client.start_server(license=\'<SUA LICEN\xc7A AQUI>\')\nDownloading MariTalk-small (path: /root/bin/maritalk)...\n/home/ubuntu/bin/maritalk: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.5G/14.5G [15:39<00:00, 15.4MB/s]\nStarting MariTalk Local API at http://localhost:9000\n>>> client.status()\n{\'status\': \'idle\'}\n>>> messages = [\n...     {"role": "user", "content": "sugira tr\xeas nomes para a minha cachorra"},\n...     {"role": "assistant", "content": "nina, bela e luna."},\n...     {"role": "user", "content": "e para o meu peixe?"},\n... ]\n>>> client.generate(messages)\n{\'output\': \'nadador, marinho e azul.\', \'queue_time\': 0, \'prompt_time\': 270, \'generation_time\': 143}\n'})}),"\n",(0,r.jsx)(a.p,{children:"Para iniciar manualmente, primeiro fa\xe7a download do execut\xe1vel que vai ser enviado por email ap\xf3s adquirir a licen\xe7a. Em seguida, execute:"}),"\n",(0,r.jsx)(a.pre,{children:(0,r.jsx)(a.code,{className:"language-console",children:'$ ./maritalk-cuda120 --license <SUA LICEN\xc7A AQUI> --max-batch-total-tokens 32768\nStarting MariTalk Local API sabia-2-small-2024-03-13...\n\u2713 Loaded in 25s\n[2024-02-22 18:20:56 +0000] Warming up...\nStart using MariTalk Local API:\n\n        $ python\n        >>> import maritalk\n        >>> client = maritalk.MariTalkLocal()\n        >>> messages = [\n                {"role": "user", "content": "sugira tr\xeas nomes para a minha cachorra"}\n                {"role": "assistant", "content": "nina, bela e luna."}\n                {"role": "user", "content": "e para o meu peixe?"}\n        ]\n        >>> client.generate_chat(messages)\n        {\'output\': \'azul, neon e dory.\', \'queue_time\': 0.224, \'generation_time\': 0.407}\n\n[2024-02-22 18:21:01 +0000] Listening on http://0.0.0.0:9000\n'})})]})}function m(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,r.jsx)(a,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>i,x:()=>s});var r=n(6540);const o={},t=r.createContext(o);function i(e){const a=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(t.Provider,{value:a},e.children)}}}]);