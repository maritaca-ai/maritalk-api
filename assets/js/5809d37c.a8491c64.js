"use strict";(self.webpackChunkmaritaca=self.webpackChunkmaritaca||[]).push([[6988],{3836:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>c,toc:()=>u});var s=t(4848),r=t(8453),a=t(1470),i=t(9365);const o={id:"responses-api",title:"Responses API"},l="Responses API",c={id:"en/responses-api",title:"Responses API",description:"The Responses API is the primary interface for interacting with Sabi\xe1 models. It provides a single, cohesive surface for generating text, calling external functions, producing structured outputs, and building multi-turn conversation flows \u2014 all through one endpoint.",source:"@site/docs/en/responses-api.md",sourceDirName:"en",slug:"/en/responses-api",permalink:"/en/responses-api",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"responses-api",title:"Responses API"},sidebar:"sidebarEn",previous:{title:"Compatibility with OpenAI",permalink:"/en/api/openai-compatibility"},next:{title:"Function Calls",permalink:"/en/function-call"}},d={},u=[{value:"Responses API vs. Chat Completions API",id:"responses-api-vs-chat-completions-api",level:3},{value:"Migrating from Chat Completions",id:"migrating-from-chat-completions",level:3},{value:"Quick start",id:"quick-start",level:2},{value:"Input formats",id:"input-formats",level:2},{value:"Simple text \u2014 for direct questions",id:"simple-text--for-direct-questions",level:3},{value:"Message list \u2014 for conversations and context",id:"message-list--for-conversations-and-context",level:3},{value:"With system instructions",id:"with-system-instructions",level:3},{value:"Streaming (real-time response)",id:"streaming-real-time-response",level:2},{value:"More examples",id:"more-examples",level:2},{value:"Example 1 \u2014 Function calling (full flow)",id:"example-1--function-calling-full-flow",level:3},{value:"Example 2 \u2014 Structured outputs with Pydantic",id:"example-2--structured-outputs-with-pydantic",level:3},{value:"Example 3 \u2014 Streaming with token tracking",id:"example-3--streaming-with-token-tracking",level:3},{value:"Example 4 \u2014 Multi-turn chatbot with memory",id:"example-4--multi-turn-chatbot-with-memory",level:3}];function p(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"responses-api",children:"Responses API"})}),"\n",(0,s.jsx)(n.p,{children:"The Responses API is the primary interface for interacting with Sabi\xe1 models. It provides a single, cohesive surface for generating text, calling external functions, producing structured outputs, and building multi-turn conversation flows \u2014 all through one endpoint."}),"\n",(0,s.jsx)(n.p,{children:"If you already use the Chat Completions API, the Responses API is its natural evolution: same compatibility with the OpenAI SDK, but with a more expressive interface and additional features that simplify complex integrations."}),"\n",(0,s.jsx)(n.h3,{id:"responses-api-vs-chat-completions-api",children:"Responses API vs. Chat Completions API"}),"\n",(0,s.jsx)(n.p,{children:"The Responses API replaces Chat Completions as the recommended interface for new projects. The table below summarizes the differences:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Aspect"}),(0,s.jsx)(n.th,{children:"Chat Completions"}),(0,s.jsx)(n.th,{children:"Responses API"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Endpoint"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"POST /v1/chat/completions"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"POST /v1/responses"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Input"}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:"messages"})," (list of messages)"]}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:"input"})," (string or list of items)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"System instructions"}),(0,s.jsxs)(n.td,{children:["Via message with ",(0,s.jsx)(n.code,{children:'role: "system"'})]}),(0,s.jsxs)(n.td,{children:["Dedicated ",(0,s.jsx)(n.code,{children:"instructions"})," parameter"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Output"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"choices[0].message.content"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"output[0].content[0].text"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Streaming"}),(0,s.jsxs)(n.td,{children:["(",(0,s.jsx)(n.code,{children:"chat.completion.chunk"}),")"]}),(0,s.jsxs)(n.td,{children:["(",(0,s.jsx)(n.code,{children:"response.output_text.delta"}),", ",(0,s.jsx)(n.code,{children:"response.completed"}),", etc.)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Function calling"}),(0,s.jsxs)(n.td,{children:["Via ",(0,s.jsx)(n.code,{children:"tool_calls"})," in message"]}),(0,s.jsxs)(n.td,{children:["Typed ",(0,s.jsx)(n.code,{children:"function_call"})," items in output"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Structured outputs"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"response_format"})}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:"text.format"})," with ",(0,s.jsx)(n.code,{children:"json_schema"})]})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"The Chat Completions API remains available and functional. If you already have a working integration with it, there's no immediate need to migrate. For new projects, we recommend the Responses API."}),"\n",(0,s.jsx)(n.h3,{id:"migrating-from-chat-completions",children:"Migrating from Chat Completions"}),"\n",(0,s.jsx)(n.p,{children:"Migration is straightforward. Compare the two equivalent examples:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Chat Completions:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'response = client.chat.completions.create(\n    model="sabia-4",\n    messages=[\n        {"role": "system", "content": "Respond concisely."},\n        {"role": "user", "content": "What is the capital of Brazil?"},\n    ],\n    max_tokens=200,\n)\nprint(response.choices[0].message.content)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Responses API:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'response = client.responses.create(\n    model="sabia-4",\n    instructions="Respond concisely.",\n    input="What is the capital of Brazil?",\n    max_output_tokens=200,\n)\nprint(response.output[0].content[0].text)\n'})}),"\n",(0,s.jsxs)(n.p,{children:["The main changes are: ",(0,s.jsx)(n.code,{children:"messages"})," becomes ",(0,s.jsx)(n.code,{children:"input"}),", the ",(0,s.jsx)(n.code,{children:"system"})," message becomes the ",(0,s.jsx)(n.code,{children:"instructions"})," parameter, ",(0,s.jsx)(n.code,{children:"max_tokens"})," becomes ",(0,s.jsx)(n.code,{children:"max_output_tokens"}),", and accessing the response changes from ",(0,s.jsx)(n.code,{children:"choices[0].message.content"})," to ",(0,s.jsx)(n.code,{children:"output[0].content[0].text"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick start"}),"\n",(0,s.jsx)(n.p,{children:"Install the OpenAI library (the Maritaca API is compatible with the OpenAI SDK):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai\n"})}),"\n",(0,s.jsx)(n.p,{children:"Make your first request:"}),"\n",(0,s.jsxs)(a.A,{children:[(0,s.jsx)(i.A,{value:"python",label:"Python",default:!0,children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\n\nclient = openai.OpenAI(\n    api_key="YOUR_API_KEY",\n    base_url="https://chat.maritaca.ai/api",\n)\n\nresponse = client.responses.create(\n    model="sabia-4",\n    input="What is the capital of Brazil?",\n)\n\nprint(response.output[0].content[0].text)\n# "The capital of Brazil is Bras\xedlia."\n'})})}),(0,s.jsx)(i.A,{value:"curl",label:"cURL",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST https://chat.maritaca.ai/api/v1/responses \\\n  -H "Authorization: Bearer YOUR_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "sabia-4",\n    "input": "What is the capital of Brazil?"\n  }\'\n'})})})]}),"\n",(0,s.jsx)(n.h2,{id:"input-formats",children:"Input formats"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"input"})," parameter accepts two formats. Choose the one that best fits your use case:"]}),"\n",(0,s.jsx)(n.h3,{id:"simple-text--for-direct-questions",children:"Simple text \u2014 for direct questions"}),"\n",(0,s.jsx)(n.p,{children:"Use when you have a single question, with no need for prior context:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'response = client.responses.create(\n    model="sabia-4",\n    input="What is the capital of Brazil?",\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"message-list--for-conversations-and-context",children:"Message list \u2014 for conversations and context"}),"\n",(0,s.jsx)(n.p,{children:"Use when you need to maintain conversation history or define roles (user, assistant):"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'response = client.responses.create(\n    model="sabia-4",\n    input=[\n        {"role": "user", "content": "My name is Alice."},\n        {"role": "assistant", "content": "Hello Alice! Nice to meet you."},\n        {"role": "user", "content": "What is my name?"},\n    ],\n)\n\nprint(response.output[0].content[0].text)  # Should mention "Alice"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Supported roles: ",(0,s.jsx)(n.code,{children:"user"})," (user), ",(0,s.jsx)(n.code,{children:"assistant"})," (model), ",(0,s.jsx)(n.code,{children:"system"})," (system instructions), and ",(0,s.jsx)(n.code,{children:"developer"})," (developer instructions)."]}),"\n",(0,s.jsx)("br",{}),"\n",(0,s.jsx)("div",{className:"custom-box",style:{display:"flex",alignItems:"center",backgroundColor:"var(--ifm-table-stripe-background)",padding:"12px",border:"1px solid var(--navbar-border)",borderRadius:"8px",margin:"12px 0",color:"var(--ifm-font-color-base)"},children:(0,s.jsxs)("div",{children:[(0,s.jsxs)("strong",{style:{display:"block",fontSize:"1em",marginBottom:"5px"},children:["Tip: Use ",(0,s.jsx)(n.code,{children:"instructions"})," to define behavior"]}),(0,s.jsxs)("p",{style:{fontSize:"0.9em"},children:["Instead of including system instructions in the message list, you can use the ",(0,s.jsx)("code",{children:"instructions"})," parameter. This separates the model's behavior from the conversation content, making your code cleaner and more organized."]})]})}),"\n",(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.h3,{id:"with-system-instructions",children:"With system instructions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'response = client.responses.create(\n    model="sabia-4",\n    instructions="You are a history teacher. Respond in a didactic way.",\n    input="Who was Dom Pedro I?",\n)\n\nprint(response.output[0].content[0].text)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"streaming-real-time-response",children:"Streaming (real-time response)"}),"\n",(0,s.jsxs)(n.p,{children:["By default, the API waits for the model to generate the entire response before returning it. With streaming, you receive the response ",(0,s.jsx)(n.strong,{children:"word by word"})," as it's generated \u2014 ideal for chat interfaces or long responses."]}),"\n",(0,s.jsxs)(a.A,{children:[(0,s.jsx)(i.A,{value:"python",label:"Python",default:!0,children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'stream = client.responses.create(\n    model="sabia-4",\n    input="Tell a brief story about Brazil.",\n    stream=True,\n)\n\nfor event in stream:\n    if event.type == "response.output_text.delta":\n        print(event.delta, end="", flush=True)\n'})})}),(0,s.jsx)(i.A,{value:"curl",label:"cURL",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST https://chat.maritaca.ai/api/v1/responses \\\n  -H "Authorization: Bearer YOUR_API_KEY" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "sabia-4",\n    "input": "Tell a brief story about Brazil.",\n    "stream": true\n  }\'\n'})})})]}),"\n",(0,s.jsx)("br",{}),"\n",(0,s.jsx)("div",{className:"custom-box",style:{display:"flex",alignItems:"center",backgroundColor:"var(--ifm-table-stripe-background)",padding:"12px",border:"1px solid var(--navbar-border)",borderRadius:"8px",margin:"12px 0",color:"var(--ifm-font-color-base)"},children:(0,s.jsxs)("div",{children:[(0,s.jsx)("strong",{style:{display:"block",fontSize:"1em",marginBottom:"5px"},children:"In practice"}),(0,s.jsxs)("p",{style:{fontSize:"0.9em"},children:["The main event is ",(0,s.jsx)("code",{children:"response.output_text.delta"}),", which contains each text fragment generated. The other events (",(0,s.jsx)("code",{children:"response.created"}),", ",(0,s.jsx)("code",{children:"response.completed"}),", etc.) are useful for advanced control, such as knowing when the response finished or tracking token usage. In case of an error, a ",(0,s.jsx)(n.code,{children:"response.failed"})," event is emitted instead of ",(0,s.jsx)(n.code,{children:"response.completed"}),"."]})]})}),"\n",(0,s.jsx)("br",{}),"\n",(0,s.jsx)(n.h2,{id:"more-examples",children:"More examples"}),"\n",(0,s.jsx)(n.h3,{id:"example-1--function-calling-full-flow",children:"Example 1 \u2014 Function calling (full flow)"}),"\n",(0,s.jsxs)(n.p,{children:["The entire function calling flow with a real API: define the tool, send a question, receive the ",(0,s.jsx)(n.code,{children:"function_call"}),", execute the function locally by calling the ",(0,s.jsx)(n.a,{href:"https://open-meteo.com/",children:"Open-Meteo API"}),", and send the result back to the model for the final response."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nimport requests\nimport json\n\nclient = openai.OpenAI(\n    api_key="YOUR_API_KEY",\n    base_url="https://chat.maritaca.ai/api",\n)\n\n# Define the function that will be executed\ndef get_weather(latitude, longitude):\n    """Calls the Open-Meteo API and returns real data."""\n    response = requests.get(\n        "https://api.open-meteo.com/v1/forecast",\n        params={\n            "latitude": latitude,\n            "longitude": longitude,\n            "current_weather": True,\n            "timezone": "America/Sao_Paulo",\n        },\n    )\n    data = response.json()\n    return {\n        "temperature": data["current_weather"]["temperature"],\n        "unit": "\xb0C",\n    }\n\ntools = [\n    {\n        "type": "function",\n        "name": "get_weather",\n        "description": "Gets the current weather forecast for a location.",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "latitude": {\n                    "type": "number",\n                    "description": "City latitude.",\n                },\n                "longitude": {\n                    "type": "number",\n                    "description": "City longitude.",\n                },\n            },\n            "required": ["latitude", "longitude"],\n        },\n    }\n]\n\nresponse = client.responses.create(\n    model="sabia-4",\n    input="What\'s the weather forecast in Rio de Janeiro?",\n    tools=tools,\n)\n\n# Verify the model requested a function call\ntool_call = response.output[0]\nassert tool_call.type == "function_call"\n\nprint(f"Function: {tool_call.name}")         # "get_weather"\nprint(f"Arguments: {tool_call.arguments}")   # \'{"latitude": -22.9068, "longitude": -43.1729}\'\nprint(f"call_id: {tool_call.call_id}")       # "call_abc123..."\n\n# Execute the function with the model\'s arguments\nargs = json.loads(tool_call.arguments)\nreal_result = get_weather(args["latitude"], args["longitude"])\nprint(f"Real API data: {real_result}")\n# {"temperature": 25.6, "unit": "\xb0C"}\n\n# Send the result back to the model for the final response\nfinal_response = client.responses.create(\n    model="sabia-4",\n    input=[\n        {"role": "user", "content": "What\'s the weather forecast in Rio de Janeiro?"},\n        {\n            "type": "function_call",\n            "call_id": tool_call.call_id,\n            "name": tool_call.name,\n            "arguments": tool_call.arguments,\n        },\n        {\n            "type": "function_call_output",\n            "call_id": tool_call.call_id,\n            "output": json.dumps(real_result),\n        },\n    ],\n    tools=tools,\n)\n\nprint(final_response.output[0].content[0].text)\n# "The weather forecast in Rio de Janeiro indicates a current temperature of 24.8\xb0C."\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-2--structured-outputs-with-pydantic",children:"Example 2 \u2014 Structured outputs with Pydantic"}),"\n",(0,s.jsxs)(n.p,{children:["Use Pydantic to define the schema, pass it via ",(0,s.jsx)(n.code,{children:"text.format"}),", and validate the result automatically."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nimport json\nfrom pydantic import BaseModel\nfrom typing import List\n\nclient = openai.OpenAI(\n    api_key="YOUR_API_KEY",\n    base_url="https://chat.maritaca.ai/api",\n)\n\n# 1. Define the schema with Pydantic\nclass Event(BaseModel):\n    name: str\n    date: str\n    location: str\n\nclass ExtractedEvents(BaseModel):\n    events: List[Event]\n\n# 2. Send with structured format\nresponse = client.responses.create(\n    model="sabia-4",\n    instructions="Extract the events mentioned in the text.",\n    input="The 2014 World Cup was held in Brazil. "\n          "The 2016 Olympics took place in Rio de Janeiro.",\n    text={\n        "format": {\n            "type": "json_schema",\n            "name": "extracted_events",\n            "schema": ExtractedEvents.model_json_schema(),\n            "strict": True,\n        }\n    },\n)\n\n# 3. Parse and validate the result\nraw_json = response.output[0].content[0].text\ndata = json.loads(raw_json)\nresult = ExtractedEvents.model_validate(data)\n\nfor event in result.events:\n    print(f"{event.name} \u2014 {event.date} in {event.location}")\n# "World Cup \u2014 2014 in Brazil"\n# "Olympics \u2014 2016 in Rio de Janeiro"\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["For more format options (",(0,s.jsx)(n.code,{children:"json_object"}),", ",(0,s.jsx)(n.code,{children:"strict"}),"), see ",(0,s.jsx)(n.a,{href:"/en/structured-outputs",children:"Structured Outputs"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-3--streaming-with-token-tracking",children:"Example 3 \u2014 Streaming with token tracking"}),"\n",(0,s.jsx)(n.p,{children:"Accumulate the generated text and capture token usage statistics at the end of the stream."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\n\nclient = openai.OpenAI(\n    api_key="YOUR_API_KEY",\n    base_url="https://chat.maritaca.ai/api",\n)\n\nstream = client.responses.create(\n    model="sabia-4",\n    input="Explain the importance of the Amazon Rainforest in 3 paragraphs.",\n    stream=True,\n)\n\nfull_text = ""\nusage = None\n\nfor event in stream:\n    if event.type == "response.output_text.delta":\n        # Accumulate generated text\n        full_text += event.delta\n        print(event.delta, end="", flush=True)\n\n    elif event.type == "response.completed":\n        # Capture token statistics\n        usage = event.response.usage\n\nprint("\\n")\nprint(f"--- Statistics ---")\nprint(f"Input tokens:   {usage.input_tokens}")\nprint(f"Output tokens:  {usage.output_tokens}")\nprint(f"Total tokens:   {usage.total_tokens}")\nprint(f"Text length:    {len(full_text)} characters")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-4--multi-turn-chatbot-with-memory",children:"Example 4 \u2014 Multi-turn chatbot with memory"}),"\n",(0,s.jsxs)(n.p,{children:["A conversation loop where the history is maintained in the ",(0,s.jsx)(n.code,{children:"input"})," list, allowing the model to remember previous messages."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\n\nclient = openai.OpenAI(\n    api_key="YOUR_API_KEY",\n    base_url="https://chat.maritaca.ai/api",\n)\n\nhistory = []\n\nprint("Chatbot (type \'exit\' to quit)")\nprint("-" * 40)\n\nwhile True:\n    user_input = input("You: ")\n    if user_input.lower() == "exit":\n        break\n\n    history.append({"role": "user", "content": user_input})\n\n    response = client.responses.create(\n        model="sabia-4",\n        instructions="You are a friendly and helpful assistant.",\n        input=history,\n    )\n\n    assistant_message = response.output[0].content[0].text\n    history.append({"role": "assistant", "content": assistant_message})\n\n    print(f"Assistant: {assistant_message}")\n'})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"The history grows with each turn. For long conversations, consider limiting the number of messages or using summaries to keep token usage under control."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},9365:(e,n,t)=>{t.d(n,{A:()=>i});t(6540);var s=t(4164);const r={tabItem:"tabItem_Ymn6"};var a=t(4848);function i(e){let{children:n,hidden:t,className:i}=e;return(0,a.jsx)("div",{role:"tabpanel",className:(0,s.A)(r.tabItem,i),hidden:t,children:n})}},1470:(e,n,t)=>{t.d(n,{A:()=>_});var s=t(6540),r=t(4164),a=t(3104),i=t(6347),o=t(205),l=t(7485),c=t(1682),d=t(679);function u(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,s.useMemo)((()=>{const e=n??function(e){return u(e).map((e=>{let{props:{value:n,label:t,attributes:s,default:r}}=e;return{value:n,label:t,attributes:s,default:r}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function h(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const r=(0,i.W6)(),a=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l.aZ)(a),(0,s.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(r.location.search);n.set(a,e),r.replace({...r.location,search:n.toString()})}),[a,r])]}function x(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,a=p(e),[i,l]=(0,s.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!h({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const s=t.find((e=>e.default))??t[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:a}))),[c,u]=m({queryString:t,groupId:r}),[x,f]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,a]=(0,d.Dv)(t);return[r,(0,s.useCallback)((e=>{t&&a.set(e)}),[t,a])]}({groupId:r}),g=(()=>{const e=c??x;return h({value:e,tabValues:a})?e:null})();(0,o.A)((()=>{g&&l(g)}),[g]);return{selectedValue:i,selectValue:(0,s.useCallback)((e=>{if(!h({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),u(e),f(e)}),[u,f,a]),tabValues:a}}var f=t(2303);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var j=t(4848);function b(e){let{className:n,block:t,selectedValue:s,selectValue:i,tabValues:o}=e;const l=[],{blockElementScrollPositionUntilNextRender:c}=(0,a.a_)(),d=e=>{const n=e.currentTarget,t=l.indexOf(n),r=o[t].value;r!==s&&(c(n),i(r))},u=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,j.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":t},n),children:o.map((e=>{let{value:n,label:t,attributes:a}=e;return(0,j.jsx)("li",{role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,ref:e=>l.push(e),onKeyDown:u,onClick:d,...a,className:(0,r.A)("tabs__item",g.tabItem,a?.className,{"tabs__item--active":s===n}),children:t??n},n)}))})}function v(e){let{lazy:n,children:t,selectedValue:a}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===a));return e?(0,s.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,j.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==a})))})}function y(e){const n=x(e);return(0,j.jsxs)("div",{className:(0,r.A)("tabs-container",g.tabList),children:[(0,j.jsx)(b,{...n,...e}),(0,j.jsx)(v,{...n,...e})]})}function _(e){const n=(0,f.A)();return(0,j.jsx)(y,{...e,children:u(e.children)},String(n))}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var s=t(6540);const r={},a=s.createContext(r);function i(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);