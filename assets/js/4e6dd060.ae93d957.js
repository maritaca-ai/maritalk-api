"use strict";(self.webpackChunkmaritaca=self.webpackChunkmaritaca||[]).push([[846],{38516:(e,a,o)=>{o.r(a),o.d(a,{assets:()=>d,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>c});var n=o(74848),s=o(28453);const r={id:"biblioteca",title:"Biblioteca"},i="Biblioteca",t={id:"pt/biblioteca",title:"Biblioteca",description:"Oferecemos a biblioteca Python da Maritalk para facilitar a integra\xe7\xe3o com a nossa API. Recomendamos o uso da vers\xe3o compat\xedvel com a OpenAI (detalhada em Compatibilidade com a OpenAI), ideal para quem j\xe1 utiliza as bibliotecas da OpenAI ou busca manter compatibilidade com essa plataforma.",source:"@site/docs/pt/biblioteca.md",sourceDirName:"pt",slug:"/pt/biblioteca",permalink:"/pt/biblioteca",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"biblioteca",title:"Biblioteca"},sidebar:"sidebarPt",previous:{title:"Compatibilidade com a OpenAI",permalink:"/pt/maritalk-api/openai-compatibilidade"},next:{title:"Casos de uso",permalink:"/pt/casos-de-uso"}},d={},c=[{value:"Instalar a biblioteca Python da Maritalk",id:"instalar-a-biblioteca-python-da-maritalk",level:2},{value:"Enviando uma solicita\xe7\xe3o para a API",id:"enviando-uma-solicita\xe7\xe3o-para-a-api",level:2},{value:"Streaming",id:"streaming",level:3},{value:"Generator",id:"generator",level:4},{value:"AsyncGenerator",id:"asyncgenerator",level:4},{value:"Modo chat",id:"modo-chat",level:2},{value:"Exemplos few-shot",id:"exemplos-few-shot",level:2}];function l(e){const a={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.header,{children:(0,n.jsx)(a.h1,{id:"biblioteca",children:"Biblioteca"})}),"\n",(0,n.jsx)(a.p,{children:"Oferecemos a biblioteca Python da Maritalk para facilitar a integra\xe7\xe3o com a nossa API. Recomendamos o uso da vers\xe3o compat\xedvel com a OpenAI (detalhada em Compatibilidade com a OpenAI), ideal para quem j\xe1 utiliza as bibliotecas da OpenAI ou busca manter compatibilidade com essa plataforma.\nPara usu\xe1rios que j\xe1 utilizavam a vers\xe3o pr\xf3pria da nossa biblioteca, mantemos essa alternativa dispon\xedvel para garantir a continuidade dos projetos existentes. Ambas as vers\xf5es compartilham as mesmas funcionalidades principais, e voc\xea pode optar pela que melhor atende ao seu fluxo de trabalho."}),"\n",(0,n.jsx)(a.h2,{id:"instalar-a-biblioteca-python-da-maritalk",children:"Instalar a biblioteca Python da Maritalk"}),"\n",(0,n.jsx)(a.p,{children:"Com o Python instalado e opcionalmente com o ambiente virtual ativado e o pip atualizado, voc\xea pode instalar a biblioteca maritalk. No terminal/linha de comando, execute:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"pip install maritalk\n"})}),"\n",(0,n.jsx)(a.p,{children:"Com o ambiente virtual ativado, voc\xea pode listar todas as bibliotecas Python instaladas nesse ambiente com o comando pip list. Abra o terminal ou prompt de comando e digite:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"pip list\n"})}),"\n",(0,n.jsx)(a.p,{children:"Se a instala\xe7\xe3o foi bem-sucedida, voc\xea ver\xe1 algo parecido com isto:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"maritalk X.X.X\n"})}),"\n",(0,n.jsx)(a.p,{children:"onde X.X.X \xe9 o n\xfamero da vers\xe3o da biblioteca maritalk que voc\xea instalou."}),"\n",(0,n.jsx)(a.h2,{id:"enviando-uma-solicita\xe7\xe3o-para-a-api",children:"Enviando uma solicita\xe7\xe3o para a API"}),"\n",(0,n.jsx)(a.p,{children:"Depois de configurar o Python e configurar uma chave API, voc\xea pode enviar uma solicita\xe7\xe3o \xe0 API da Maritalk usando a biblioteca Python. Para fazer isso, crie um arquivo chamado maritalk.py usando o terminal ou um IDE.\nDentro do arquivo, copie e cole um dos exemplos abaixo:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'import maritalk\n\nmodel = maritalk.MariTalk(\n    key="insira sua chave aqui. Ex: \'100088...\'",\n    model="sabia-3"  # No momento, suportamos os modelos sabia-3 e sabia-2-small\n)\n\nresponse = model.generate("Quanto \xe9 25 + 27?", max_tokens=8000)\nanswer = response["answer"]\n\nprint(f"Resposta: {answer}")   # Deve imprimir algo como "25 + 27 \xe9 igual a 52."\n'})}),"\n",(0,n.jsx)(a.p,{children:"Note que o dicion\xe1rio response cont\xe9m a chave usage, que informa a quantidade de tokens de entrada e sa\xedda que ser\xe3o cobrados.\nPara executar o c\xf3digo, digite python maritalk.py no terminal/linha de comando."}),"\n",(0,n.jsx)(a.h3,{id:"streaming",children:"Streaming"}),"\n",(0,n.jsx)(a.p,{children:"Para tarefas de gera\xe7\xe3o de texto longo, como a cria\xe7\xe3o de um artigo extenso ou a tradu\xe7\xe3o de um documento grande, pode ser vantajoso receber a resposta em partes, \xe0 medida que o texto \xe9 gerado, em vez de esperar pelo texto completo. Isso torna a aplica\xe7\xe3o mais responsiva e eficiente, especialmente quando o texto gerado \xe9 extenso. Oferecemos duas abordagens para atender a essa necessidade: o uso de um generator e de um async_generator."}),"\n",(0,n.jsx)(a.h4,{id:"generator",children:"Generator"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Ao usar ",(0,n.jsx)(a.code,{children:"stream=True"}),", o c\xf3digo ir\xe1 retornar um ",(0,n.jsx)(a.code,{children:"generator"}),". Este ",(0,n.jsx)(a.code,{children:"generator"})," fornecer\xe1 as partes da resposta conforme elas s\xe3o geradas pelo modelo, permitindo que voc\xea imprima ou processe os tokens \xe0 medida que s\xe3o produzidos."]}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:"for response in model.generate(\n    messages,\n    do_sample=True,\n    max_tokens=200,\n    temperature=0.7,\n    top_p=0.95,\n    stream=True,\n    num_tokens_per_message=4\n):\n    print(response)\n"})}),"\n",(0,n.jsx)(a.h4,{id:"asyncgenerator",children:"AsyncGenerator"}),"\n",(0,n.jsxs)(a.p,{children:["Ao utilizar ",(0,n.jsx)(a.code,{children:"stream=True"})," em conjunto com ",(0,n.jsx)(a.code,{children:"return_async_generator=True"}),", o c\xf3digo ir\xe1 retornar um ",(0,n.jsx)(a.code,{children:"AsyncGenerator"}),". Este tipo de gerador \xe9 projetado para ser consumido de forma ass\xedncrona, o que significa que voc\xea pode executar o c\xf3digo que consome o ",(0,n.jsx)(a.code,{children:"AsyncGenerator"})," de maneira concorrente com outras tarefas, melhorando a efici\xeancia do seu processamento."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:"import asyncio\n\nasync_generator = model.generate(\n    messages,\n    do_sample=True,\n    max_tokens=200,\n    temperature=0.7,\n    top_p=0.95,\n    stream=True,\n    return_async_generator=True,\n    num_tokens_per_message=4\n)\n\nasync def consume_generator():\n    async for response in async_generator:\n        print(response)\n                # Seu c\xf3digo aqui...\n\nasyncio.run(consume_generator)\n"})}),"\n",(0,n.jsx)(a.h2,{id:"modo-chat",children:"Modo chat"}),"\n",(0,n.jsxs)(a.p,{children:["Voc\xea pode definir uma conversa especificando uma lista de dicion\xe1rios, sendo que cada dicion\xe1rio precisar ter duas chaves: ",(0,n.jsx)(a.code,{children:"content"})," e ",(0,n.jsx)(a.code,{children:"role"}),"."]}),"\n",(0,n.jsxs)(a.p,{children:["Atualmente, a API da Maritaca suporta tr\xeas valores para ",(0,n.jsx)(a.code,{children:"role"}),': "system" para mensagem de instru\xe7\xe3o do chatbot, "user" para mensagens do usu\xe1rio, e "assistant" para mensagens do assistente.']}),"\n",(0,n.jsx)(a.p,{children:"Mostramos um exemplo de conversa abaixo:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:'messages = [\n    {"role": "user", "content": "sugira tr\xeas nomes para a minha cachorra"},\n    {"role": "assistant", "content": "nina, bela e luna."},\n    {"role": "user", "content": "e para o meu peixe?"},\n]\n\nanswer = model.generate(\n    messages,\n    do_sample=True,\n    max_tokens=200,\n    temperature=0.7,\n    top_p=0.95)["answer"]\n\nprint(f"Resposta: {answer}")   # Deve imprimir algo como "nemo, dory e neptuno."\n'})}),"\n",(0,n.jsx)(a.h2,{id:"exemplos-few-shot",children:"Exemplos few-shot"}),"\n",(0,n.jsx)(a.p,{children:"Embora o Sabi\xe1 seja capaz de responder a instru\xe7\xf5es sem nenhum exemplo de demonstra\xe7\xe3o, fornecer alguns exemplos da tarefa pode melhorar significativamente a qualidade de suas respostas."}),"\n",(0,n.jsx)(a.p,{children:"Abaixo mostramos como isso \xe9 feito para uma tarefa simples de an\xe1lise de sentimento, i.e., classificar se uma resenha de filme \xe9 positiva ou negativa.\nNeste caso, passaremos dois exemplos few-shot, um positivo e outro negativo, e um terceiro exemplo, para o qual o Sabi\xe1 efetivamente far\xe1 a predi\xe7\xe3o."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'prompt = """Classifique a resenha de filme como "positiva" ou "negativa".\n\nResenha: Gostei muito do filme, \xe9 o melhor do ano!\nClasse: positiva\n\nResenha: O filme deixa muito a desejar.\nClasse: negativa\n\nResenha: Apesar de longo, valeu o ingresso..\nClasse:"""\n\nanswer = model.generate(\n    prompt,\n    chat_mode=False,\n    do_sample=False,\n    max_tokens=20,\n    stopping_tokens=["\\n"]\n)["answer"]\n\nprint(f"Resposta: {answer.strip()}")  # Deve imprimir "positiva"\n'})}),"\n",(0,n.jsxs)(a.p,{children:["Note que usamos ",(0,n.jsx)(a.code,{children:"chat_mode=False"}),", pois melhora a qualidade das respostas quando usando exemplos few-shot."]}),"\n",(0,n.jsxs)(a.p,{children:["O argumento ",(0,n.jsx)(a.code,{children:'stopping_tokens=["\\n"]'}),' \xe9 usado para interromper a gera\xe7\xe3o quando o token "\\n" \xe9 gerado. Isso \xe9 necess\xe1rio porque, quando n\xe3o estamos no modo chat, o modelo pode n\xe3o saber quando interromper a gera\xe7\xe3o.']}),"\n",(0,n.jsxs)(a.p,{children:["Para tarefas com apenas uma resposta correta, como no exemplo acima, \xe9 recomendado usar ",(0,n.jsx)(a.code,{children:"do_sample=False"}),". Isso garante que a mesma resposta seja gerada dado um prompt espec\xedfico."]}),"\n",(0,n.jsxs)(a.p,{children:["Para tarefas de gera\xe7\xe3o de textos diversos ou longos, \xe9 recomendado usar ",(0,n.jsx)(a.code,{children:"do_sample=True"})," e ",(0,n.jsx)(a.code,{children:"temperature=0.7"}),'. Quanto maior a temperatura, mais diversos ser\xe3o os textos gerados, mas h\xe1 maior chance de o modelo "alucinar" e gerar textos sem sentido. Quanto menor a temperatura, a resposta \xe9 mais conservadora, mas corre o risco de gerar textos repetidos.']})]})}function m(e={}){const{wrapper:a}={...(0,s.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}},28453:(e,a,o)=>{o.d(a,{R:()=>i,x:()=>t});var n=o(96540);const s={},r=n.createContext(s);function i(e){const a=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function t(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),n.createElement(r.Provider,{value:a},e.children)}}}]);