"use strict";(self.webpackChunkmaritaca=self.webpackChunkmaritaca||[]).push([[47],{5627:(e,a,s)=>{s.r(a),s.d(a,{assets:()=>p,contentTitle:()=>m,default:()=>j,frontMatter:()=>c,metadata:()=>h,toc:()=>u});var n=s(4848),o=s(8453),r=(s(6540),s(8774));const i={grid:"grid_LGBJ",card:"card_KDiI",icon:"icon_UkIS"};var l=s(3914),t=s(6188),d=s(7875);const c={id:"maritalk-local-introducao",title:"Introdu\xe7\xe3o"},m="Maritalk Local",h={id:"pt/maritalk-local/maritalk-local-introducao",title:"Introdu\xe7\xe3o",description:'Al\xe9m de usar nossos modelos via API, voc\xea pode tamb\xe9m hosped\xe1-los localmente. Seus dados nunca saem da sua m\xe1quina local; apenas informa\xe7\xf5es sobre o tempo de uso do modelo s\xe3o enviados para nossos servidores para efeitos de cobran\xe7a. A pol\xedtica de licenciamento da \\"Maritalk Local\\" permite a execu\xe7\xe3o de multiplas inst\xe2ncias simultaneamente por licen\xe7a, com valor cobrado proporcional ao n\xfamero de inst\xe2ncias ativas. Os usu\xe1rios n\xe3o ficam limitados ao hardware, ou seja, podem trocar quantas vezes desejarem, sem restri\xe7\xf5es.',source:"@site/docs/pt/maritalk-local/maritalk-local-introducao.md",sourceDirName:"pt/maritalk-local",slug:"/pt/maritalk-local/maritalk-local-introducao",permalink:"/pt/maritalk-local/maritalk-local-introducao",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"maritalk-local-introducao",title:"Introdu\xe7\xe3o"},sidebar:"sidebarPt",previous:{title:"Casos de Uso",permalink:"/pt/casos-de-uso"},next:{title:"Google Cloud",permalink:"/pt/maritalk-local/google-cloud"}},p={},u=[{value:"Obtendo uma licen\xe7a",id:"obtendo-uma-licen\xe7a",level:2},{value:"Exemplos",id:"exemplos",level:2},{value:"Desempenho",id:"desempenho",level:2},{value:"Resultados",id:"resultados",level:3},{value:"Detalhes",id:"detalhes",level:3}];function x(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components},{Details:s}=a;return s||function(e,a){throw new Error("Expected "+(a?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.header,{children:(0,n.jsx)(a.h1,{id:"maritalk-local",children:"Maritalk Local"})}),"\n",(0,n.jsx)(a.p,{children:'Al\xe9m de usar nossos modelos via API, voc\xea pode tamb\xe9m hosped\xe1-los localmente. Seus dados nunca saem da sua m\xe1quina local; apenas informa\xe7\xf5es sobre o tempo de uso do modelo s\xe3o enviados para nossos servidores para efeitos de cobran\xe7a. A pol\xedtica de licenciamento da "Maritalk Local" permite a execu\xe7\xe3o de multiplas inst\xe2ncias simultaneamente por licen\xe7a, com valor cobrado proporcional ao n\xfamero de inst\xe2ncias ativas. Os usu\xe1rios n\xe3o ficam limitados ao hardware, ou seja, podem trocar quantas vezes desejarem, sem restri\xe7\xf5es.'}),"\n",(0,n.jsx)(a.h2,{id:"obtendo-uma-licen\xe7a",children:"Obtendo uma licen\xe7a"}),"\n",(0,n.jsxs)(a.p,{children:["O primeiro passo para usar a Maritalk Local \xe9 obter uma licen\xe7a ",(0,n.jsx)(a.a,{href:"https://www.maritaca.ai/#maritalk-local",children:"aqui"}),"."]}),"\n",(0,n.jsx)(a.p,{children:"O n\xfamero da licen\xe7a ser\xe1 enviado via email. Ap\xf3s, siga um dos exemplos abaixo para fazer download e come\xe7ar a usar o modelo adquirido."}),"\n",(0,n.jsx)(a.h2,{id:"exemplos",children:"Exemplos"}),"\n",(0,n.jsxs)("div",{className:i.grid,children:[(0,n.jsxs)(r.A,{to:"google-cloud",className:i.card,children:[(0,n.jsx)(l.g,{icon:t.jBL,className:i.icon}),(0,n.jsx)("h3",{children:"Google Cloud Platform"}),(0,n.jsx)("p",{children:"Veja como fazer o deploy da MariTalk Local na GCP."})]}),(0,n.jsxs)(r.A,{to:"oracle-cloud",className:i.card,children:[(0,n.jsx)(l.g,{icon:t.D6w,className:i.icon}),(0,n.jsx)("h3",{children:"Oracle Cloud Infrastructure"}),(0,n.jsx)("p",{children:"Veja como fazer o deploy da MariTalk Local na OCI."})]}),(0,n.jsxs)(r.A,{to:"docker",className:i.card,children:[(0,n.jsx)(l.g,{icon:d._yG,className:i.icon}),(0,n.jsx)("h3",{children:"Docker"}),(0,n.jsx)("p",{children:"Veja como rodar a MariTalk Local em containers Docker."})]}),(0,n.jsxs)(r.A,{to:"https://github.com/maritaca-ai/maritalk-api/blob/main/examples/local/colab-pro.ipynb",className:i.card,children:[(0,n.jsx)(l.g,{icon:t.fPr,className:i.icon}),(0,n.jsx)("h3",{children:"Google Colab Pro"}),(0,n.jsx)("p",{children:"Veja como usar a MariTalk Local no Google Colab Pro."})]})]}),"\n",(0,n.jsx)(a.h2,{id:"desempenho",children:"Desempenho"}),"\n",(0,n.jsx)(a.p,{children:"Para a vers\xe3o Sabi\xe1-2 Small rodando em uma m\xe1quina com 1xA100 40GB, \xe9 esperado um throughput de ~118 tokens/s, considerando uma entrada de 500 tokens e sa\xedda de 100 tokens. Ou seja, uma requisi\xe7\xe3o com esse tamanho de entrada e sa\xedda demora cerca de 5 segundos para finalizar. Em um cen\xe1rio com um prompt de 1.000 tokens e 500 tokens de sa\xedda, este tempo \xe9 cerca de 27 segundos. Conforme o n\xfamero de requisi\xe7\xf5es simult\xe2neas aumenta, \xe9 esperado ver um aumento no throughput total do sistema e um aumento no tempo de resposta do ponto de vista de cada requisi\xe7\xe3o isoladamente."}),"\n",(0,n.jsx)(a.p,{children:"Na vers\xe3o Sabi\xe1-2 Medium, rodando em uma m\xe1quina com 2xA100 40GB, a mesma requisi\xe7\xe3o com 500 tokens de entrada e 100 tokens de sa\xedda demoraria cerca de 10 segundos."}),"\n",(0,n.jsxs)(a.p,{children:["Disponibilizamos ",(0,n.jsx)(a.a,{href:"https://github.com/maritaca-ai/maritalk-api/blob/main/examples/local/benchmark.py",children:"esta ferramenta de benchmark"})," para que voc\xea possa avaliar o desempenho no seu ambiente. Abaixo est\xe3o os resultados obtidos na vers\xe3o atual em 23 de fevereiro de 2024."]}),"\n",(0,n.jsx)(a.h3,{id:"resultados",children:"Resultados"}),"\n",(0,n.jsxs)(a.table,{children:[(0,n.jsx)(a.thead,{children:(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.th,{children:(0,n.jsx)(a.strong,{children:"GPU"})}),(0,n.jsx)(a.th,{children:(0,n.jsx)(a.strong,{children:"Modelo"})}),(0,n.jsx)(a.th,{children:(0,n.jsx)(a.strong,{children:"Mem\xf3ria necess\xe1ria"})}),(0,n.jsx)(a.th,{children:(0,n.jsx)(a.strong,{children:"Total (token/s)"})}),(0,n.jsx)(a.th,{children:(0,n.jsx)(a.strong,{children:"Gera\xe7\xe3o (token/s)"})})]})}),(0,n.jsxs)(a.tbody,{children:[(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"1xA100 40GB"}),(0,n.jsx)(a.td,{children:"Sabi\xe1-2 Small"}),(0,n.jsx)(a.td,{children:"20GB"}),(0,n.jsx)(a.td,{children:"167.2"}),(0,n.jsx)(a.td,{children:"42.2"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"2xA100 40GB"}),(0,n.jsx)(a.td,{children:"Sabi\xe1-2 Small"}),(0,n.jsx)(a.td,{children:"20GB"}),(0,n.jsx)(a.td,{children:"213.5"}),(0,n.jsx)(a.td,{children:"54.0"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"1xA10 24GB"}),(0,n.jsx)(a.td,{children:"Sabi\xe1-2 Small"}),(0,n.jsx)(a.td,{children:"20GB"}),(0,n.jsx)(a.td,{children:"89.8"}),(0,n.jsx)(a.td,{children:"21.3"})]}),(0,n.jsxs)(a.tr,{children:[(0,n.jsx)(a.td,{children:"2xA100 40GB"}),(0,n.jsx)(a.td,{children:"Sabi\xe1-2 Medium"}),(0,n.jsx)(a.td,{children:"70GB"}),(0,n.jsx)(a.td,{children:"79.3"}),(0,n.jsx)(a.td,{children:"18.6"})]})]})]}),"\n",(0,n.jsx)(a.h3,{id:"detalhes",children:"Detalhes"}),"\n",(0,n.jsxs)(s,{children:[(0,n.jsx)("summary",{children:(0,n.jsx)("b",{children:"Sabi\xe1-2 Small (GPU 1xA100 40GB)"})}),(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Total tokens: 167.2 tokens/s"}),"\n",(0,n.jsx)(a.li,{children:"Generated tokens: 42.2 tokens/s"}),"\n"]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-bash",children:"$ python benchmark.py --concurrency 1,2,4,8 --n-repeats 5 --prompt-size 550 --max-tokens 150\n            generated_tps             total_tps\n                     mean median  std      mean median  std\nconcurrency\n1                    42.2   42.2  0.3     167.1  167.2  0.7\n2                    24.2   24.2  1.0     101.4  101.4  0.9\n4                    13.0   13.2  1.0      56.5   57.1  2.5\n8                     7.1    7.2  0.6      30.8   30.9  0.6\n\nSystem tokens\n             median   std\nconcurrency\n1             167.2   0.7\n2             202.9   0.1\n4             230.4  10.3\n8             245.8  11.9\n"})}),(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:"https://github.com/maritaca-ai/maritalk-api/assets/1206395/7acfb1c6-b2a2-40e4-b6e7-2ed08201819d",alt:"benchmark-small-1xa100"})})]}),"\n",(0,n.jsxs)(s,{children:[(0,n.jsx)("summary",{children:(0,n.jsx)("b",{children:"Sabi\xe1-2 Small (GPU 2xA100 40GB)"})}),(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Total tokens: 213.5 tokens/s"}),"\n",(0,n.jsx)(a.li,{children:"Generated tokens: 54.0 tokens/s"}),"\n"]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-console",children:"$ python benchmark.py --concurrency 1,2,4,8 --n-repeats 5 --prompt-size 550 --max-tokens 150\n            generated_tps             total_tps\n                     mean median  std      mean median   std\nconcurrency\n1                    54.0   53.6  0.8     213.5  208.1  12.3\n2                    33.2   33.1  1.4     135.6  135.6   1.2\n4                    20.1   20.6  1.3      85.2   85.6   1.2\n8                    11.1   11.2  0.9      48.1   48.3   0.9\n\nSystem tokens\n             median   std\nconcurrency\n1             208.1  12.3\n2             271.3   0.3\n4             340.8   0.8\n8             384.7   1.0\n"})}),(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:"https://github.com/maritaca-ai/maritalk-api/assets/1206395/524a0b74-7998-4f24-928d-61ae803b98eb",alt:"benchmark-small-2xa100"})})]}),"\n",(0,n.jsxs)(s,{children:[(0,n.jsx)("summary",{children:(0,n.jsx)("b",{children:"Sabi\xe1-2 Small (GPU 1xA10 24GB)"})}),(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Total tokens: 89.8 tokens/s"}),"\n",(0,n.jsx)(a.li,{children:"Generated tokens: 21.3 tokens/s"}),"\n"]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-console",children:"$ python benchmark.py --concurrency 1,2,4,8 --n-repeats 5 --prompt-size 550 --max-tokens 150\n            generated_tps             total_tps\n                     mean median  std      mean median  std\nconcurrency\n1                    21.3   21.3  0.2      89.8   88.6  2.2\n2                    11.3   11.2  0.4      47.7   48.0  0.9\n4                     5.8    5.9  0.3      24.4   24.5  0.3\n8                     2.9    2.9  0.2      12.2   12.2  0.2\n\nSystem tokens\n             median  std\nconcurrency\n1              88.6  2.2\n2              96.4  1.9\n4              97.6  0.2\n8              97.5  0.2\n"})}),(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:"https://github.com/maritaca-ai/maritalk-api/assets/1206395/524a0b74-7998-4f24-928d-61ae803b98eb",alt:"benchmark-small-2xa100"})})]}),"\n",(0,n.jsxs)(s,{children:[(0,n.jsx)("summary",{children:(0,n.jsx)("b",{children:"Sabi\xe1-2 Medium (GPU 2xA100 40GB)"})}),(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Total tokens: 79.3 tokens/s"}),"\n",(0,n.jsx)(a.li,{children:"Generated tokens: 18.6 tokens/s"}),"\n"]}),(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-console",children:"$ python benchmark.py --concurrency 1,2,4,8 --n-repeats 5 --prompt-size 550 --max-tokens 150 --tokenizer maritaca-ai/maritalk-tokenizer-large\n            generated_tps             total_tps\n                     mean median  std      mean median  std\nconcurrency\n1                    18.6   18.6  0.3      79.3   78.9  1.0\n2                    10.4   10.5  0.4      44.9   45.0  0.6\n4                     5.8    5.8  0.2      25.5   25.5  0.2\n8                     3.1    3.1  0.2      13.6   13.7  0.2\n\nSystem tokens\n             median  std\nconcurrency\n1              78.9  1.0\n2              90.1  1.1\n4             101.9  0.1\n8             108.9  0.2\n"})}),(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{src:"https://github.com/maritaca-ai/maritalk-api/assets/1206395/a379f94b-4472-4eeb-b166-d262bf853a1c",alt:"benchmark-medium-2xa100"})})]})]})}function j(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(x,{...e})}):x(e)}}}]);