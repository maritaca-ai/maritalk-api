"use strict";(self.webpackChunkmaritaca=self.webpackChunkmaritaca||[]).push([[6612],{2070:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var i=n(4848),a=n(8453);const s={id:"batch-api",title:"Batch API"},r="What is it?",l={id:"en/batch-api",title:"Batch API",description:"The Batch API is an efficient option for sending batches of asynchronous requests, offering cost reductions of up to 50%. In this mode, rate limits are calculated by characters per day, even though billing still occurs per token. For tiers 1 through 5, the daily limit is 1.2 billion characters (roughly 300 million tokens), while for Tier 0 the limit is 4 million characters (about 1 million tokens) per day. Only input tokens count toward the rate limit.",source:"@site/docs/en/batch_api.md",sourceDirName:"en",slug:"/en/batch-api",permalink:"/en/batch-api",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"batch-api",title:"Batch API"},sidebar:"sidebarEn",previous:{title:"Rate Limits",permalink:"/en/rate-limits"},next:{title:"Library",permalink:"/en/library"}},o={},c=[{value:"Visual interface",id:"visual-interface",level:2},{value:"Programmatic use",id:"programmatic-use",level:2},{value:"1. Setting Up Your Batch File",id:"1-setting-up-your-batch-file",level:3},{value:"2. Uploading the Batch Input File",id:"2-uploading-the-batch-input-file",level:3},{value:"3. Creating the Batch",id:"3-creating-the-batch",level:3},{value:"4. Checking the Batch Status",id:"4-checking-the-batch-status",level:3},{value:"Batch Status",id:"batch-status",level:4},{value:"5. Obtaining the Batch Results",id:"5-obtaining-the-batch-results",level:3},{value:"Handling the JSONL Output",id:"handling-the-jsonl-output",level:4},{value:"6. Canceling a Batch",id:"6-canceling-a-batch",level:3},{value:"7. Listing All Batches",id:"7-listing-all-batches",level:3},{value:"Understanding Batch Expiration",id:"understanding-batch-expiration",level:4}];function h(e){const t={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"what-is-it",children:"What is it?"})}),"\n",(0,i.jsx)(t.p,{children:"The Batch API is an efficient option for sending batches of asynchronous requests, offering cost reductions of up to 50%. In this mode, rate limits are calculated by characters per day, even though billing still occurs per token. For tiers 1 through 5, the daily limit is 1.2 billion characters (roughly 300 million tokens), while for Tier 0 the limit is 4 million characters (about 1 million tokens) per day. Only input tokens count toward the rate limit."}),"\n",(0,i.jsx)(t.p,{children:"Additionally, requests may take up to 24 hours to complete, making the service particularly suitable for workloads that do not require immediate responses and that aim to reduce operational costs. Because requests can take up to 24 hours to process and batches expire if they are not finished within that window, the Batch API is not recommended for critical scenarios where execution failure due to expiration would be unacceptable."}),"\n",(0,i.jsx)(t.p,{children:"Batch processing is often useful in cases such as:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Translation or mass text review"}),"\n",(0,i.jsx)(t.li,{children:"Performing evaluations"}),"\n",(0,i.jsx)(t.li,{children:"Classifying large datasets"}),"\n",(0,i.jsx)(t.li,{children:"Summarizing multiple documents"}),"\n",(0,i.jsx)(t.li,{children:"Extracting entities and keywords in documents"}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"The Batch API consists of a set of endpoints that allow you to:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["Create a new batch (",(0,i.jsx)(t.code,{children:"POST /batches"}),"): Start batch processing by sending a set of requests at once."]}),"\n",(0,i.jsxs)(t.li,{children:["Check the batch status (",(0,i.jsx)(t.code,{children:"GET /batches/{id}/status"}),"): Returns the processing progress, indicating whether it is still running or has finished."]}),"\n",(0,i.jsxs)(t.li,{children:["Retrieve results (",(0,i.jsx)(t.code,{children:"GET /batches/{id}/results"}),"): Provides the answers to each request in the batch as soon as the processing is completed."]}),"\n"]}),"\n",(0,i.jsx)(t.h1,{id:"how-to-use",children:"How to use"}),"\n",(0,i.jsx)(t.p,{children:"You can work with the Batch API in two ways: through the visual interface or programmatically (code)."}),"\n",(0,i.jsx)(t.h2,{id:"visual-interface",children:"Visual interface"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsx)(t.li,{children:"In the web interface, the process for uploading your .jsonl file starts in the side-menu: select Files and, in the upper-right corner of the page, click + Upload. A modal window opens where you can drag-and-drop or browse for the file on your computer\u2014just drop the .jsonl into the dashed area in the centre of the dialog. After selecting the file, press Upload to start the transfer. When the upload finishes, the document appears in the file list with the status processed and receives a file_id; you\u2019ll use this identifier to create the batch in the next step. Each file can be at most 200 MB and contain up to 50,000 requests."}),"\n"]}),"\n",(0,i.jsx)("img",{src:"/img/Batch0.png",alt:"Sabia",style:{width:"100rem",height:"auto",marginRight:"15px"}}),"\n",(0,i.jsxs)(t.ol,{start:"2",children:["\n",(0,i.jsx)(t.li,{children:"To create the batch in the web interface, open the side-menu and select Batch API; the batch list is shown in the main panel. In the upper-right corner, click + Create to open the Create Batch dialog. Simply paste the File ID generated in the previous step (the file must be under 200 MB or 50,000 requests) and then press Create Batch. The system validates the file and, once execution begins, the new batch appears in the list with its identifier and real-time progress."}),"\n"]}),"\n",(0,i.jsx)("img",{src:"/img/Batch1.png",alt:"Sabia",style:{width:"100rem",height:"auto",marginRight:"15px"}}),"\n",(0,i.jsx)(t.h2,{id:"programmatic-use",children:"Programmatic use"}),"\n",(0,i.jsx)(t.h3,{id:"1-setting-up-your-batch-file",children:"1. Setting Up Your Batch File"}),"\n",(0,i.jsxs)(t.p,{children:["For each batch, use a single ",(0,i.jsx)(t.code,{children:".jsonl"})," file: each line corresponds to one API request (the same parameters as the endpoint). Include a unique ",(0,i.jsx)(t.code,{children:"custom_id"})," in each request to locate the result later. Each file can be at most 200 MB and contain up to 50,000 requests. Example (two requests):"]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.em,{children:"Note:"})," Each file can only contain requests for a single model."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-json",children:'{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "sabia-3", "messages": [{"role": "system", "content": "Voc\xea \xe9 um assistente \xfatil"},{"role": "user", "content": "Ol\xe1 mundo!"}],"max_tokens": 100}}\n{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions","body": {"model": "sabia-3", "messages": [{"role": "system", "content": "Voc\xea \xe9 um assistente \xfatil"},{"role": "user", "content": "Ol\xe1 mundo!"}],"max_tokens": 100}}\n'})}),"\n",(0,i.jsx)(t.h3,{id:"2-uploading-the-batch-input-file",children:"2. Uploading the Batch Input File"}),"\n",(0,i.jsx)(t.p,{children:"Send your .jsonl file using the endpoint below. Files can contain at most 50,000 requests and must be no larger than 200MB each."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = openai.OpenAI(\n    api_key="",  # Your API_KEY\n    base_url="https://chat.maritaca.ai/api",\n)\n\nbatch_input_file = client.files.create(\n    file=open("batchinput.jsonl", "rb"),\n    purpose="batch"\n)\n\nprint(batch_input_file)\n'})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Note:"}),' Currently, only the "batch" purpose is supported for file uploads. Other purposes (such as "finetuning" and "assistant") are not supported.']}),"\n",(0,i.jsx)(t.h3,{id:"3-creating-the-batch",children:"3. Creating the Batch"}),"\n",(0,i.jsx)(t.p,{children:"Use the file ID (for example, file1) to create the batch. The completion_window is fixed at 24h, and you can include extra metadata via the metadata parameter. Example:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from openai import OpenAI\nclient = openai.OpenAI(\n    api_key="",  # Your API_KEY\n    base_url="https://chat.maritaca.ai/api",\n)\n\nbatch_input_file_id = batch_input_file.id\nclient.batches.create(\n    input_file_id=batch_input_file_id,\n    endpoint="/v1/chat/completions",\n    completion_window="24h",\n    metadata={\n        "description": "job for marketing data",\n        "owner": "marketing team"\n    }\n)\n'})}),"\n",(0,i.jsx)(t.p,{children:"This request will return a Batch object with metadata about your batch:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-json",children:'{\n  "id": "batch1",\n  "object": "batch",\n  "endpoint": "/v1/chat/completions",\n  "errors": null,\n  "input_file_id": "file1",\n  "completion_window": "24h",\n  "status": "validating",\n  "output_file_id": null,\n  "error_file_id": null,\n  "created_at": 1744838747,\n  "in_progress_at": null,\n  "expires_at": 1744838747,\n  "completed_at": null,\n  "failed_at": null,\n  "expired_at": null,\n  "request_counts": {\n    "total": 0,\n    "completed": 0,\n    "failed": 0\n  },\n  "metadata": null\n}\n'})}),"\n",(0,i.jsx)(t.h3,{id:"4-checking-the-batch-status",children:"4. Checking the Batch Status"}),"\n",(0,i.jsx)(t.p,{children:"It's possible to check a batch's status at any time, and you'll also receive the corresponding Batch object as part of that process."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from openai import OpenAI\nclient = openai.OpenAI(\n    api_key="",  # Your API_KEY\n    base_url="https://chat.maritaca.ai/api",\n)\n\nbatch = client.batches.retrieve("batch1")\nprint(batch)\n'})}),"\n",(0,i.jsx)(t.h4,{id:"batch-status",children:"Batch Status"}),"\n",(0,i.jsx)(t.p,{children:"Each Batch object can have one of the following status:"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:(0,i.jsx)(t.strong,{children:"Status"})}),(0,i.jsx)(t.th,{children:(0,i.jsx)(t.strong,{children:"Descri\xe7\xe3o"})})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"validating"}),(0,i.jsx)(t.td,{children:"The input file is in the process of being validated before starting the batch."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"failed"}),(0,i.jsx)(t.td,{children:"The validation process for the input file did not succeed."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"in_progress"}),(0,i.jsx)(t.td,{children:"The input file was successfully validated, and the batch is currently running."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"finalizing"}),(0,i.jsx)(t.td,{children:"The batch has finished running, and its results are now being prepared."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"completed"}),(0,i.jsx)(t.td,{children:"The batch has finished running, and the results are ready."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"expired"}),(0,i.jsx)(t.td,{children:"The batch did not complete within the 24-hour window and has expired."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"cancelling"}),(0,i.jsx)(t.td,{children:"A request has been made to cancel the batch, this can take up to 10 minutes to finalize."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"cancelled"}),(0,i.jsx)(t.td,{children:"The batch was canceled and will not continue processing."})]})]})]}),"\n",(0,i.jsx)(t.h3,{id:"5-obtaining-the-batch-results",children:"5. Obtaining the Batch Results"}),"\n",(0,i.jsx)(t.p,{children:"When the batch has finished processing, you can obtain the output by sending a request to the API using the output_file_id from the Batch object. After receiving the file, save it locally (for example, as batch_output.jsonl)."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from openai import OpenAI\nclient = openai.OpenAI(\n    api_key="",  # Your# API_KEY\n    base_url="https://chat.maritaca.ai/api",\n)\nfile_response = client.files.content("file1")\nprint(file_response.text)\n'})}),"\n",(0,i.jsx)(t.h4,{id:"handling-the-jsonl-output",children:"Handling the JSONL Output"}),"\n",(0,i.jsx)(t.p,{children:"The JSONL output file will contain a single response line for each successful request in your input file. Any requests that fail will have their error details stored in a separate error file, whose ID can be found in the Batch object\u2019s error_file_id field."}),"\n",(0,i.jsx)(t.p,{children:"Keep in mind that the order of the output lines may not match the input lines. To reliably match each output to its corresponding input, use the custom_id field, which is included in every line of the output file."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-jsonl",children:'{"id": "batch_req_1", "custom_id": "request1", "response": {"status_code": 200, "request_id": "req_1", "body": {"id": "chat1", "object": "chat.completion", "created": 1744838747, "model": "sabia-3", "choices": [{"index": 0, "message": {"role": "assistant", "content": "Ol\xe1 mundo."}, "logprobs": null, "finish_reason": "stop"}], "usage": {"prompt_tokens": 5, "completion_tokens": 14, "total_tokens": 19}, "system_fingerprint": "abc123"}}, "error": null}\n{"id": "batch_req_1", "custom_id": "request2", "response": {"status_code": 200, "request_id": "req_2", "body": {"id": "chat2", "object": "chat.completion", "created": 1744838747, "model": "sabia-3", "choices": [{"index": 0, "message": {"role": "assistant", "content": "Hello World."}, "logprobs": null, "finish_reason": "stop"}], "usage": {"prompt_tokens": 4, "completion_tokens": 26, "total_tokens": 30}, "system_fingerprint": "abc123"}}, "error": null}\n'})}),"\n",(0,i.jsx)(t.p,{children:"After the batch finishes, the output file remains available for 30 days and is then automatically deleted."}),"\n",(0,i.jsx)(t.h3,{id:"6-canceling-a-batch",children:"6. Canceling a Batch"}),"\n",(0,i.jsx)(t.p,{children:"If needed, you can cancel a batch that\u2019s currently running. During cancellation, the batch status changes to canceling while any ongoing requests continue (potentially up to 10 minutes). Once these requests have finished, the status updates to cancelled."}),"\n",(0,i.jsx)(t.p,{children:"Canceling a batch:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'\nfrom openai import OpenAI\nclient = openai.OpenAI(\n    api_key="",  # Your API_KEY\n    base_url="https://chat.maritaca.ai/api",\n)\n\nclient.batches.cancel("batch1")\n'})}),"\n",(0,i.jsx)(t.h3,{id:"7-listing-all-batches",children:"7. Listing All Batches"}),"\n",(0,i.jsx)(t.p,{children:"You can view all of your batches at any point in time. If you have a large number of batches, you can use the limit and after parameters to paginate your results."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from openai import OpenAI\nclient = openai.OpenAI(\n    api_key="",  # Your API_KEY\n    base_url="https://chat.maritaca.ai/api",\n)\n\nclient.batches.list(limit=10)\n'})}),"\n",(0,i.jsx)(t.h4,{id:"understanding-batch-expiration",children:"Understanding Batch Expiration"}),"\n",(0,i.jsx)(t.p,{children:"If a batch does not finish within the allotted time, it will transition to the expired state. Any incomplete requests are canceled, while completed requests remain available through the batch\u2019s output file. You will still be charged for the token usage of any requests that were successfully completed."}),"\n",(0,i.jsx)(t.p,{children:"Requests that expire are added to your error file with a message like the one shown below. You can use the custom_id to retrieve request details for those expired requests."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-jsonl",children:'{"id": "batch_req_1", "custom_id": "request-1", "response": null, "error": {"code": "batch_expired", "message": "This request could not be executed before the completion window expired."}}\n{"id": "batch_req_1", "custom_id": "request-2", "response": null, "error": {"code": "batch_expired", "message": "This request could not be executed before the completion window expired."}}\n'})})]})}function d(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>l});var i=n(6540);const a={},s=i.createContext(a);function r(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);