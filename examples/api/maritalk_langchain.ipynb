{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/maritaca-ai/maritalk-api/blob/add-notebook/examples/api/maritalk_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Integrando a MariTalk com o LangChain\n",
        "\n",
        "Esse notebook mostra como usar a MariTalk com o LangChain através de dois exemplos:\n",
        "\n",
        "1. Exemplo simples de como usar a MariTalk para resolver uma tarefa.\n",
        "\n",
        "2. LLM + RAG: O segundo exemplo mostra como responder uma pergunta cuja resposta se encontra em um documento longo, que não cabe no limite de tokens da MariTalk. Para isso iremos usar um buscador simples (BM25) para primeiramente buscar no documento os trechos mais relevantes e depois alimentá-los para a MariTalk responder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmXR8RAm9bIa"
      },
      "source": [
        "## Instalando as bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFIZZLpCUkej"
      },
      "outputs": [],
      "source": [
        "!pip install maritalk -q\n",
        "!pip install langchain[all] -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDlDpael9d7S"
      },
      "source": [
        "# Implementação do MariTalk no LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GY9QTI9UHK0"
      },
      "outputs": [],
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain_core.language_models.chat_models import BaseChatModel, SimpleChatModel\n",
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        ")\n",
        "import maritalk\n",
        "\n",
        "class ChatMaritalk(SimpleChatModel):\n",
        "    api_key: str\n",
        "    temperature: float = 0.7\n",
        "    chat_mode: bool = True\n",
        "    max_tokens: int = 512\n",
        "    do_sample: bool = True\n",
        "    top_p: float = 0.95\n",
        "    system_message_workaround: bool = True\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"maritalk\"\n",
        "\n",
        "    def parse_messages_for_model(self, messages: List[BaseMessage]):\n",
        "        parsed_messages=[]\n",
        "\n",
        "        for message in messages:\n",
        "            if isinstance(message, HumanMessage):\n",
        "                parsed_messages.append({\"role\":\"user\",\"content\":message.content})\n",
        "            elif isinstance(message, AIMessage):\n",
        "                parsed_messages.append({\"role\":\"assistant\",\"content\":message.content})\n",
        "            elif isinstance(message, SystemMessage):\n",
        "                if self.system_message_workaround:\n",
        "                    parsed_messages.append({\"role\":\"user\",\"content\":message.content})\n",
        "                    parsed_messages.append({\"role\":\"assistant\",\"content\": \"ok\"})\n",
        "\n",
        "        return parsed_messages\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        model = maritalk.MariTalk(key=self.api_key)\n",
        "        stop_tokens=stop if stop is not None else []\n",
        "        messages=self.parse_messages_for_model(messages)\n",
        "        answer = model.generate(\n",
        "            messages,\n",
        "            temperature=self.temperature,\n",
        "            max_tokens=self.max_tokens,\n",
        "            stopping_tokens=stop_tokens,\n",
        "            do_sample=self.do_sample,\n",
        "            top_p=self.top_p,\n",
        "            chat_mode=True,\n",
        "        )\n",
        "\n",
        "        return answer\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\"system_message_workaround\": self.system_message_workaround, \"temperature\": self.temperature, \"top_p\": self.top_p, \"max_tokens\": self.max_tokens }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3upgz2M59izJ"
      },
      "source": [
        "# Exemplo 1 - Sugestão de animais de estimação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVgok_fsUyY3"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm=ChatMaritalk(\n",
        "        api_key=\"\",  # Insira aqui sua chave de API, que pode ser obtiva em chat.maritaca.ai -> \"Chaves da API\"\n",
        "        temperature=0.7,\n",
        "        chat_mode=False,\n",
        "        max_tokens=100,\n",
        "    )\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhd_4YsVVlOR"
      },
      "outputs": [],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"Você é uma assistente especialista em sugerir nome de animais de estimação. Dado o animal, você deve sugerir 4 nomes.\"\"\"),\n",
        "        (\"human\", \"\"\"eu tenho um {animal}\"\"\"),\n",
        "    ])\n",
        "\n",
        "chain = chat_prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "i8-h9sFbW6Cs",
        "outputId": "81f72f05-82ad-47ae-ddf8-ef81e4d5fa2c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. Max\\n2. Bella\\n3. Charlie\\n4. Rocky'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chain.invoke({\"animal\": \"cachorro\"})\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RU--deXrYaPU",
        "outputId": "6500fda0-1e56-4d81-84d4-e0e81540ae14"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. Nemo\\n2. Dory\\n3. Aqua\\n4. Finny'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chain.invoke({\"animal\": \"peixe\"})\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaSwS8Ap9u1Q"
      },
      "source": [
        "# Exemplo 2 - Sistema de perguntas e respostas do vestibular da UNICAMP 2024\n",
        "\n",
        "Para este segundo exemplo, é necessario instalar algumas bibliotecas extras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install unstructured rank_bm25 pdf2image pdfminer-six pikepdf pypdf unstructured_inference fastapi kaleido uvicorn \"pillow<10.1.0\" pillow_heif -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAny0NjW-LmX"
      },
      "source": [
        "## Carregando o edital\n",
        "\n",
        "O primeiro passo é criar uma base de dados com as informações do edital. Para isso vamos baixar o edital do site da comvest e segmentar o texto extraído em janelas de 500 caracteres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjeVsEUni693"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import OnlinePDFLoader\n",
        "\n",
        "# Carregando o edital da comvest 2024\n",
        "loader = OnlinePDFLoader(\"https://www.comvest.unicamp.br/wp-content/uploads/2023/10/31-2023-Dispoe-sobre-o-Vestibular-Unicamp-2024_com-retificacao.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, separators=['\\n', ' ', ''])\n",
        "texts = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTlBSkXE-Y33"
      },
      "source": [
        "## Criando um buscador\n",
        "\n",
        "Agora que ja temos nossa base de dados, precisamos de um buscador. Para esse exemplo usaremos um simples BM25 como sistema de busca, mas este poderia ser substituido por qualquer outro buscador (como por exemplo busca via embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STS1pcxU1hMH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "retriever = BM25Retriever.from_documents(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7OAqPs7-vwR"
      },
      "source": [
        "## Unindo Sistema Busca + LLM\n",
        "\n",
        "Agora que ja temos nosso buscador, basta implementarmos um prompt especificando a tarefa e invocar a cadeia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yESgCCcg17gP"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "prompt = \"\"\"Baseado nos seguintes documentos, responda a pergunta abaixo.\n",
        "\n",
        "{context}\n",
        "\n",
        "Pergunta: {query}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt)])\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True, prompt=qa_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7HvDsZ417Vn",
        "outputId": "0d28c534-a5f6-464b-9133-db5072669445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: \n",
            "Baseado nos seguintes documentos, responda a pergunta\n",
            "\n",
            "§4º O(a) candidato(a) terá no máximo 5 (cinco) horas e no mínimo 2 (duas) horas para a realização das provas estabelecidas para cada dia. Poderá ser concedido tempo adicional aos(às) candidatos(as) nos casos previstos no art. 14.\n",
            "\n",
            "§5º A ausência ou a obtenção de nota 0 (zero) em qualquer uma das provas, exceto nas provas de Habilidades Específicas, eliminará o(a) candidato(a) do VU 2024.\n",
            "\n",
            "Art. 19 A 1ª fase será constituída de uma única prova de Conhecimentos Gerais composta por 72 (setenta e duas) questões objetivas sobre as áreas do conhecimento desenvolvidas no Ensino Médio, incluindo questões interdisciplinares.\n",
            "\n",
            "§1º O(a) candidato(a) terá no máximo 5 (cinco) horas e no mínimo 2 duas horas para a realização da prova da 1ª fase. Poderá ser concedido tempo adicional aos(às) candidatos(as) nos casos previstos no art. 14.\n",
            "\n",
            "§12 Os(as) candidatos(as) com deficiência ou em condições que exijam recursos específicos poderão ser atendidos(as), a partir de critérios definidos pela Comvest, das seguintes formas:\n",
            "\n",
            "a. Através de caderno de questões com letra ampliada. b. Com auxílio para transcrição. c. Com maior tempo para a realização da prova, tempo este estabelecido de acordo com\n",
            "\n",
            "critérios neuropsicológicos, até o limite de 20% do tempo regular.\n",
            "\n",
            "e. Através de outros recursos, a depender da necessidade comprovada.\n",
            "\n",
            "§13 O(a) candidato(a) que necessitar de tempo adicional de até 20% do tempo regulamentar em cada dia de realização do exame deverá declarar e comprovar, no processo de inscrição, ser pessoa com deficiência ou ter outra condição especial.\n",
            "\n",
            "Pergunta: Qual o tempo máximo para realização da prova?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_documents': [Document(page_content='§4º O(a) candidato(a) terá no máximo 5 (cinco) horas e no mínimo 2 (duas) horas para a realização das provas estabelecidas para cada dia. Poderá ser concedido tempo adicional aos(às) candidatos(as) nos casos previstos no art. 14.\\n\\n§5º A ausência ou a obtenção de nota 0 (zero) em qualquer uma das provas, exceto nas provas de Habilidades Específicas, eliminará o(a) candidato(a) do VU 2024.', metadata={'source': '/tmp/tmprm_mreyk/tmp.pdf'}),\n",
              "  Document(page_content='Art. 19 A 1ª fase será constituída de uma única prova de Conhecimentos Gerais composta por 72 (setenta e duas) questões objetivas sobre as áreas do conhecimento desenvolvidas no Ensino Médio, incluindo questões interdisciplinares.\\n\\n§1º O(a) candidato(a) terá no máximo 5 (cinco) horas e no mínimo 2 duas horas para a realização da prova da 1ª fase. Poderá ser concedido tempo adicional aos(às) candidatos(as) nos casos previstos no art. 14.', metadata={'source': '/tmp/tmprm_mreyk/tmp.pdf'}),\n",
              "  Document(page_content='§12 Os(as) candidatos(as) com deficiência ou em condições que exijam recursos específicos poderão ser atendidos(as), a partir de critérios definidos pela Comvest, das seguintes formas:\\n\\na. Através de caderno de questões com letra ampliada. b. Com auxílio para transcrição. c. Com maior tempo para a realização da prova, tempo este estabelecido de acordo com\\n\\ncritérios neuropsicológicos, até o limite de 20% do tempo regular.', metadata={'source': '/tmp/tmprm_mreyk/tmp.pdf'}),\n",
              "  Document(page_content='e. Através de outros recursos, a depender da necessidade comprovada.\\n\\n§13 O(a) candidato(a) que necessitar de tempo adicional de até 20% do tempo regulamentar em cada dia de realização do exame deverá declarar e comprovar, no processo de inscrição, ser pessoa com deficiência ou ter outra condição especial.', metadata={'source': '/tmp/tmprm_mreyk/tmp.pdf'})],\n",
              " 'query': 'Qual o tempo máximo para realização da prova?',\n",
              " 'output_text': 'O tempo máximo para realização da prova é de 5 horas.'}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Qual o tempo máximo para realização da prova?\"\n",
        "\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "chain.invoke({\"input_documents\": docs, \"query\": query})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
